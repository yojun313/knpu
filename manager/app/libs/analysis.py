import asyncio
import warnings
from PyQt5.QtWidgets import QMessageBox
import pandas as pd
import sys
import matplotlib.pyplot as plt
import seaborn as sns
import platform
from wordcloud import WordCloud
from collections import Counter
import os
import csv
from googletrans import Translator
from tqdm import tqdm
from ui.status import printStatus
from PIL import Image
from core.setting import get_setting
import re
from libs.path import safe_path
from libs.console import closeConsole

Image.MAX_IMAGE_PIXELS = None  # í¬ê¸° ì œí•œ í•´ì œ
warnings.filterwarnings("ignore")

# ìš´ì˜ì²´ì œì— ë”°ë¼ í•œê¸€ í°íŠ¸ë¥¼ ì„¤ì •
if platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Windows':  # Windows
    plt.rcParams['font.family'] = 'Malgun Gothic'  # ë§‘ì€ ê³ ë”• í°íŠ¸ ì‚¬ìš©

# í°íŠ¸ ì„¤ì • í›„ ìŒìˆ˜ ê¸°í˜¸ê°€ ê¹¨ì§€ëŠ” ê²ƒì„ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False


class DataProcess:

    def __init__(self, main_window):
        self.main = main_window
        
    def checkColumns(self, required_columns, columns):
        # 2. ëˆ„ë½ëœ ì»¬ëŸ¼ í™•ì¸
        missing_columns = [col for col in required_columns if col not in columns]

        if missing_columns:
            closeConsole()
            QMessageBox.warning(
                self.main,
                "Warning",
                f"ë‹¤ìŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n{', '.join(missing_columns)}\n\n"
                f"CSV íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
            return False
        return True

    def TimeSplitter(self, data):
        # data í˜•íƒœ: DataFrame
        data_columns = data.columns.tolist()

        for i in data_columns:
            if 'Date' in i:
                word = i
                break

        data[word] = pd.to_datetime(
            data[word], format='%Y-%m-%d', errors='coerce')

        data['year'] = data[word].dt.year
        data['month'] = data[word].dt.month
        data['year_month'] = data[word].dt.to_period('M')
        data['week'] = data[word].dt.to_period('W')

        return data

    def TimeSplitToCSV(self, option, divided_group, data_path, tablename):
        # í´ë” ì´ë¦„ê³¼ ë°ì´í„° ê·¸ë£¹ ì„¤ì •
        data_group = divided_group
        if option == 1:
            folder_name = "Year Data"
            info_label = 'Year'
        elif option == 2:
            folder_name = "Month Data"
            info_label = 'Month'
        elif option == 3:
            folder_name = "Week Data"
            info_label = 'Week'

        info = {}

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.mkdir(data_path + "/" + folder_name)

        # ë°ì´í„° ê·¸ë£¹ì„ ìˆœíšŒí•˜ë©° íŒŒì¼ ì €ì¥ ë° ì •ë³´ ìˆ˜ì§‘
        for group_name, group_data in data_group:
            info[str(group_name)] = len(group_data)
            group_data.to_csv(f"{data_path}/{folder_name}/{tablename+'_'+str(group_name)}.csv",
                              index=False, encoding='utf-8-sig', header=True)

        # ì •ë³´ íŒŒì¼ ìƒì„±
        info_df = pd.DataFrame(list(info.items()), columns=[
                               info_label, 'Count'])
        info_df.to_csv(f"{data_path}/{folder_name}/{folder_name} Count.csv",
                       index=False, encoding='utf-8-sig', header=True)

        info_df.set_index(info_label, inplace=True)
        keys = list(info_df.index)
        values = info_df['Count'].tolist()

        # ë°ì´í„°ì˜ ìˆ˜ì— ë”°ë¼ ê·¸ë˜í”„ í¬ê¸° ìë™ ì¡°ì •
        num_data_points = len(keys)
        width_per_data_point = 0.5  # ë°ì´í„° í¬ì¸íŠ¸ í•˜ë‚˜ë‹¹ ê°€ë¡œ í¬ê¸° (ì¡°ì • ê°€ëŠ¥)
        base_width = 10  # ìµœì†Œ ê°€ë¡œ í¬ê¸°
        height = 6  # ê³ ì •ëœ ì„¸ë¡œ í¬ê¸°

        fig_width = max(base_width, num_data_points * width_per_data_point)

        plt.figure(figsize=(fig_width, height))

        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        sns.lineplot(x=keys, y=values, marker='o')

        # ê·¸ë˜í”„ ì„¤ì •
        plt.grid(True)
        plt.xticks(rotation=45)
        plt.tight_layout()

        plt.title(f'{info_label} Data Visualization')
        plt.xlabel(info_label)
        plt.ylabel('Values')

        # ê·¸ë˜í”„ ì €ì¥
        plt.savefig(
            f"{data_path}/{folder_name}/{folder_name} Graph.png", bbox_inches='tight')

    def calculate_figsize(self, data_length, base_width=12, height=6, max_width=50):
        # Increase width proportionally to the number of data points, but limit the maximum width
        width = min(base_width + (data_length / 20), max_width)
        return (width, height)

    def NaverNewsArticleAnalysis(self, data, file_path):
        if not self.checkColumns([
            "Article Press",
            "Article Type", 
            "Article URL",
            "Article Title", 
            "Article Text", 
            "Article Date", 
            "Article ReplyCnt"
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))
            

        # 'Article Date'ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        data['Article Date'] = pd.to_datetime(
            data['Article Date'], errors='coerce')
        # 'Article ReplyCnt' ì—´ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ê³ , ë³€í™˜ì´ ì•ˆ ë˜ëŠ” ê°’ì€ NaNìœ¼ë¡œ ì²˜ë¦¬
        data['Article ReplyCnt'] = pd.to_numeric(
            data['Article ReplyCnt'], errors='coerce').fillna(0)

        # ê¸°ë³¸ í†µê³„ ë¶„ì„
        basic_stats = data.describe(include='all')

        # ì‹œê°„ì— ë”°ë¥¸ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        time_analysis = data.groupby(data['Article Date'].dt.to_period("M")).agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()
        time_analysis['Article Date'] = time_analysis['Article Date'].dt.to_timestamp()

        # ì‹œê°„ì— ë”°ë¥¸ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (ì¼ë³„)
        day_analysis = data.groupby(data['Article Date'].dt.to_period("D")).agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()
        day_analysis['Article Date'] = day_analysis['Article Date'].dt.to_timestamp()

        # ê¸°ì‚¬ ìœ í˜•ë³„ ë¶„ì„
        article_type_analysis = data.groupby('Article Type').agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()

        # ì–¸ë¡ ì‚¬ë³„ ë¶„ì„ (ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ë§Œ)
        top_10_press = data['Article Press'].value_counts().head(10).index
        press_analysis = data[data['Article Press'].isin(top_10_press)].groupby('Article Press').agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()

        # ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ)
        numeric_columns = ['Article ReplyCnt']
        correlation_matrix = data[numeric_columns].corr()

        # ì‹œê°í™” ë° ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = os.path.join(os.path.dirname(file_path),
                                  os.path.basename(file_path).replace('.csv', '') + '_analysis')
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding='utf-8-sig')
        time_analysis.to_csv(os.path.join(
            csv_output_dir, "time_analysis.csv"), encoding='utf-8-sig', index=False)
        day_analysis.to_csv(os.path.join(
            csv_output_dir, "day_analysis.csv"), encoding='utf-8-sig', index=False)
        article_type_analysis.to_csv(os.path.join(csv_output_dir, "article_type_analysis.csv"), encoding='utf-8-sig',
                                     index=False)
        press_analysis.to_csv(os.path.join(
            csv_output_dir, "press_analysis.csv"), encoding='utf-8-sig', index=False)
        # correlation_matrix.to_csv(os.path.join(output_dir, "correlation_matrix.csv"), encoding='utf-8-sig', index=False)

        # For time_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(time_analysis)))
        sns.lineplot(data=time_analysis, x='Article Date',
                     y='Article Count', label='Article Count')
        sns.lineplot(data=time_analysis, x='Article Date',
                     y='Article ReplyCnt', label='Reply Count')
        plt.title('Monthly Article and Reply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "monthly_article_reply_count.png"))
        plt.close()

        # For day_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(day_analysis)))
        sns.lineplot(data=day_analysis, x='Article Date',
                     y='Article Count', label='Article Count')
        sns.lineplot(data=day_analysis, x='Article Date',
                     y='Article ReplyCnt', label='Reply Count')
        plt.title('Daily Article and Reply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "daily_article_reply_count.png"))
        plt.close()

        # For article_type_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(article_type_analysis)))
        article_type_analysis = article_type_analysis.sort_values(
            'Article Count', ascending=False)
        sns.barplot(x='Article Type', y='Article Count',
                    data=article_type_analysis, palette="viridis")
        plt.title('Article Count by Type')
        plt.xlabel('Article Type')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "article_type_count.png"))
        plt.close()

        # For press_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(press_analysis)))
        press_analysis = press_analysis.sort_values(
            'Article Count', ascending=False)
        sns.barplot(x='Article Press', y='Article Count',
                    data=press_analysis, palette="plasma")
        plt.title('Top 10 Press by Article Count')
        plt.xlabel('Press')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "press_article_count.png"))
        plt.close()

        # ê·¸ë˜í”„ ì„¤ëª… ì‘ì„± (í•œêµ­ì–´)
        description_text = """
        ê·¸ë˜í”„ ì„¤ëª…:

        1. ì›”ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (monthly_article_reply_count.png):
           - ì´ ì„  ê·¸ë˜í”„ëŠ” ì‹œê°„ì— ë”°ë¥¸ ì›”ë³„ ê¸°ì‚¬ ìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ ìˆ˜ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ ê·¸ë˜í”„ëŠ” íŠ¹ì • ê¸°ê°„ ë™ì•ˆ ê¸°ì‚¬ì™€ ëŒ“ê¸€ì´ ì–´ë–»ê²Œ ë³€ë™í–ˆëŠ”ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

        2. ê¸°ì‚¬ ìœ í˜•ë³„ ë¶„ì„ (article_type_count.png):
           - ì´ ë§‰ëŒ€ ê·¸ë˜í”„ëŠ” ê¸°ì‚¬ ìœ í˜•ë³„ ê¸°ì‚¬ ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ê¸°ì‚¬ ìœ í˜•ì„, yì¶•ì€ í•´ë‹¹ ìœ í˜•ì˜ ê¸°ì‚¬ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ ê·¸ë˜í”„ëŠ” ì–´ë–¤ ìœ í˜•ì˜ ê¸°ì‚¬ê°€ ê°€ì¥ ë§ì´ ë°œí–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

        3. ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ìˆ˜ (press_article_count.png):
           - ì´ ë§‰ëŒ€ ê·¸ë˜í”„ëŠ” ê¸°ì‚¬ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ì–¸ë¡ ì‚¬ëª…ì„, yì¶•ì€ ê° ì–¸ë¡ ì‚¬ì—ì„œ ë°œí–‰í•œ ê¸°ì‚¬ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ ê·¸ë˜í”„ëŠ” ê°€ì¥ í™œë°œí•˜ê²Œ ê¸°ì‚¬ë¥¼ ë°œí–‰í•˜ëŠ” ì–¸ë¡ ì‚¬ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.
        
        4. ì¼ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (daily_article_reply_count.png):
           - ì´ ì„  ê·¸ë˜í”„ëŠ” ì‹œê°„ì— ë”°ë¥¸ ì¼ë³„ ê¸°ì‚¬ ìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ ìˆ˜ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - íŠ¹ì • ì¼ì— ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ê°€ ê¸‰ì¦í•˜ëŠ” íŒ¨í„´ì„ íŒŒì•…í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
        """

        # ì„¤ëª…ì„ txt íŒŒì¼ë¡œ ì €ì¥
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def NaverNewsStatisticsAnalysis(self, data, file_path):
        if not self.checkColumns([
            "Article Press", 
            "Article Type", 
            "Article URL", 
            "Article Title", 
            "Article Text",
            "Article Date", 
            "Article ReplyCnt",
            "Male", 
            "Female",
            "10Y", 
            "20Y", 
            "30Y", 
            "40Y", 
            "50Y", 
            "60Y"
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 'Article Date'ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ NaTë¡œ ë³€í™˜)
        data['Article Date'] = pd.to_datetime(
            data['Article Date'], errors='coerce')

        # 'Article ReplyCnt'ë¥¼ ìˆ«ì(float)ë¡œ ë³€í™˜
        data['Article ReplyCnt'] = pd.to_numeric(
            data['Article ReplyCnt'], errors='coerce').fillna(0)

        # ë°±ë¶„ìœ¨ ê°’ì„ ì‹¤ì œ ëŒ“ê¸€ ìˆ˜ë¡œ ë³€í™˜í•˜ê¸° ì „ì— ê° ì—´ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ê³ , ë³€í™˜ ë¶ˆê°€ ì‹œ 0ìœ¼ë¡œ ì±„ì›€
        for col in ['Male', 'Female', '10Y', '20Y', '30Y', '40Y', '50Y', '60Y']:
            data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)
            data[col] = (data[col] / 100.0) * data['Article ReplyCnt']

        # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = os.path.join(os.path.dirname(file_path),
                                  os.path.basename(file_path).replace('.csv', '') + '_analysis')
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # ê¸°ë³¸ í†µê³„ ë¶„ì„
        basic_stats = data.describe(include='all')

        # ì‹œê°„ì— ë”°ë¥¸ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        time_analysis = data.groupby(data['Article Date'].dt.to_period("M")).agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()
        time_analysis['Article Date'] = time_analysis['Article Date'].dt.to_timestamp()

        # ì‹œê°„ì— ë”°ë¥¸ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (ì¼ë³„)
        day_analysis = data.groupby(data['Article Date'].dt.to_period("D")).agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()
        day_analysis['Article Date'] = day_analysis['Article Date'].dt.to_timestamp()

        # ê¸°ì‚¬ ìœ í˜•ë³„ ë¶„ì„
        article_type_analysis = data.groupby('Article Type').agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()

        # ì–¸ë¡ ì‚¬ë³„ ë¶„ì„ (ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ë§Œ)
        top_10_press = data['Article Press'].value_counts().head(10).index
        press_analysis = data[data['Article Press'].isin(top_10_press)].groupby('Article Press').agg({
            'id': 'count',
            'Article ReplyCnt': 'sum'
        }).rename(columns={'id': 'Article Count'}).reset_index()

        # ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ)
        numeric_columns = ['Article ReplyCnt', 'Male',
                           'Female', '10Y', '20Y', '30Y', '40Y', '50Y', '60Y']
        correlation_matrix = data[numeric_columns].corr()

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding='utf-8-sig')
        time_analysis.to_csv(os.path.join(
            csv_output_dir, "time_analysis.csv"), encoding='utf-8-sig', index=False)
        day_analysis.to_csv(os.path.join(
            csv_output_dir, "day_analysis.csv"), encoding='utf-8-sig', index=False)
        article_type_analysis.to_csv(os.path.join(csv_output_dir, "article_type_analysis.csv"), encoding='utf-8-sig',
                                     index=False)
        press_analysis.to_csv(os.path.join(
            csv_output_dir, "press_analysis.csv"), encoding='utf-8-sig', index=False)
        correlation_matrix.to_csv(os.path.join(csv_output_dir, "correlation_matrix.csv"), encoding='utf-8-sig',
                                  index=False)

        # ì‹œê°í™” ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥

        # 1. ì›”ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸
        plt.figure(figsize=self.calculate_figsize(len(time_analysis)))
        sns.lineplot(data=time_analysis, x='Article Date',
                     y='Article Count', label='Article Count')
        sns.lineplot(data=time_analysis, x='Article Date',
                     y='Article ReplyCnt', label='Reply Count')
        plt.title('Monthly Article and Reply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "monthly_article_reply_count.png"))
        plt.close()

        # For day_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(day_analysis)))
        sns.lineplot(data=day_analysis, x='Article Date',
                     y='Article Count', label='Article Count')
        sns.lineplot(data=day_analysis, x='Article Date',
                     y='Article ReplyCnt', label='Reply Count')
        plt.title('Daily Article and Reply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "daily_article_reply_count.png"))
        plt.close()

        # 2. ê¸°ì‚¬ ìœ í˜•ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜
        plt.figure(figsize=self.calculate_figsize(len(article_type_analysis)))
        article_type_analysis = article_type_analysis.sort_values(
            'Article Count', ascending=False)
        sns.barplot(x='Article Type', y='Article Count',
                    data=article_type_analysis, palette="viridis")
        plt.title('Article Count by Type')
        plt.xlabel('Article Type')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "article_type_count.png"))
        plt.close()

        # 3. ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜
        plt.figure(figsize=self.calculate_figsize(len(press_analysis)))
        press_analysis = press_analysis.sort_values(
            'Article Count', ascending=False)
        sns.barplot(x='Article Press', y='Article Count',
                    data=press_analysis, palette="plasma")
        plt.title('Top 10 Press by Article Count')
        plt.xlabel('Press')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "press_article_count.png"))
        plt.close()

        # 4. ìƒê´€ê´€ê³„ í–‰ë ¬ íˆíŠ¸ë§µ
        plt.figure(figsize=self.calculate_figsize(
            len(correlation_matrix), height=8))
        sns.heatmap(correlation_matrix, annot=True,
                    cmap='coolwarm', vmin=-1, vmax=1)
        plt.title('Correlation Matrix of Key Metrics')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "correlation_matrix.png"))
        plt.close()

        # 5. ì„±ë³„ ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        gender_reply_count = {
            'Male': data['Male'].sum(), 'Female': data['Female'].sum()}
        gender_reply_df = pd.DataFrame(list(gender_reply_count.items()), columns=[
                                       'Gender', 'Reply Count'])
        plt.figure(figsize=self.calculate_figsize(
            len(gender_reply_df), base_width=8))
        sns.barplot(x='Gender', y='Reply Count',
                    data=gender_reply_df, palette="pastel")
        plt.title('Total Number of Replies by Gender')
        plt.xlabel('Gender')
        plt.ylabel('Reply Count')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "gender_reply_count.png"))
        plt.close()
        gender_reply_df.to_csv(os.path.join(csv_output_dir, "gender_reply_count.csv"), index=False,
                               encoding='utf-8-sig')

        # 6. ì—°ë ¹ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        age_group_reply_count = {age: data[age].sum() for age in [
            '10Y', '20Y', '30Y', '40Y', '50Y', '60Y']}
        age_group_reply_df = pd.DataFrame(list(age_group_reply_count.items()), columns=[
                                          'Age Group', 'Reply Count'])
        plt.figure(figsize=self.calculate_figsize(
            len(age_group_reply_df), base_width=10))
        sns.barplot(x='Age Group', y='Reply Count',
                    data=age_group_reply_df, palette="coolwarm")
        plt.title('Total Number of Replies by Age Group')
        plt.xlabel('Age Group')
        plt.ylabel('Reply Count')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "age_group_reply_count.png"))
        plt.close()
        age_group_reply_df.to_csv(os.path.join(csv_output_dir, "age_group_reply_count.csv"), index=False,
                                  encoding='utf-8-sig')

        # ê·¸ë˜í”„ ì„¤ëª… ì‘ì„± (í•œêµ­ì–´)
        description_text = """
        ê·¸ë˜í”„ ì„¤ëª…:

        1. ì›”ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸ (monthly_article_reply_count.png):
           - ì›”ë³„ ê¸°ì‚¬ ìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

        2. ê¸°ì‚¬ ìœ í˜•ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ (article_type_count.png):
           - ê¸°ì‚¬ ìœ í˜•ë³„ ê¸°ì‚¬ì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

        3. ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ (press_article_count.png):
           - ìƒìœ„ 10ê°œ ì–¸ë¡ ì‚¬ì—ì„œ ì‘ì„±í•œ ê¸°ì‚¬ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

        4. ìƒê´€ê´€ê³„ í–‰ë ¬ íˆíŠ¸ë§µ (correlation_matrix.png):
           - ì£¼ìš” ì§€í‘œë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•œ íˆíŠ¸ë§µì…ë‹ˆë‹¤.

        5. ì„±ë³„ ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (gender_reply_count.png):
           - ë‚¨ì„±ê³¼ ì—¬ì„±ì˜ ì´ ëŒ“ê¸€ ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

        6. ì—°ë ¹ëŒ€ë³„ ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (age_group_reply_count.png):
           - ê° ì—°ë ¹ëŒ€ë³„ ì´ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           
        7. ì¼ë³„ ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ ë¶„ì„ (daily_article_reply_count.png):
           - ì´ ì„  ê·¸ë˜í”„ëŠ” ì‹œê°„ì— ë”°ë¥¸ ì¼ë³„ ê¸°ì‚¬ ìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ ìˆ˜ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - íŠ¹ì • ì¼ì— ê¸°ì‚¬ ë° ëŒ“ê¸€ ìˆ˜ê°€ ê¸‰ì¦í•˜ëŠ” íŒ¨í„´ì„ íŒŒì•…í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
        """

        # ì„¤ëª…ì„ txt íŒŒì¼ë¡œ ì €ì¥
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def NaverNewsReplyAnalysis(self, data, file_path):
        
        if not self.checkColumns([
            'Reply Date',
            'Reply Text',
            'Reply Writer',
            'Rereply Count',
            'Reply Like',
            'Reply Bad',
            'Reply LikeRatio',
            'Reply Sentiment'
        ], data.columns):
            return False

        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 'Reply Date'ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        data['Reply Date'] = pd.to_datetime(
            data['Reply Date'], errors='coerce')

        # ê° ì—´ì„ ìˆ«ìë¡œ ë³€í™˜
        numeric_cols = ['Rereply Count', 'Reply Like', 'Reply Bad', 'Reply LikeRatio', 'Reply Sentiment']
        optional_cols = ['TotalUserComment', 'TotalUserReply', 'TotalUserLike']

        for col in numeric_cols + [c for c in optional_cols if c in data.columns]:
            data[col] = pd.to_numeric(data[col], errors='coerce')

        # Reply Text ì—´ì´ ë¬¸ìì—´ì´ ì•„ë‹Œ ê°’ì´ ìˆê±°ë‚˜ NaNì¼ ê²½ìš° ëŒ€ë¹„
        data['Reply Text'] = data['Reply Text'].astype(str).fillna('')

        # ëŒ“ê¸€ ê¸¸ì´ ì¶”ê°€
        data['Reply Length'] = data['Reply Text'].apply(
            lambda x: len(x) if isinstance(x, str) else 0)

        # ê¸°ë³¸ í†µê³„ ë¶„ì„
        basic_stats = data.describe(include='all')

        # ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        time_analysis = data.groupby(data['Reply Date'].dt.date).agg({
            'id': 'count',
            'Reply Like': 'sum',
            'Reply Bad': 'sum'
        }).rename(columns={'id': 'Reply Count'})

        # ì›”ë³„ ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš”, ì‹«ì–´ìš” í•©ê³„ ë¶„ì„
        month_analysis = data.groupby(data['Reply Date'].dt.to_period("M")).agg({
            'id': 'count',
            'Reply Like': 'sum',
            'Reply Bad': 'sum'
        }).rename(columns={'id': 'Reply Count'}).reset_index()
        month_analysis['Reply Date'] = month_analysis['Reply Date'].dt.to_timestamp()

        # ëŒ“ê¸€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë¹ˆë„
        sentiment_counts = data['Reply Sentiment'].value_counts()

        # ìƒê´€ê´€ê³„ ë¶„ì„
        correlation_matrix = data[
            ['Reply Like', 'Reply Bad', 'Rereply Count', 'Reply LikeRatio', 'Reply Sentiment', 'Reply Length']].corr()

        # ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ê³„ì‚°
        writer_reply_count = data['Reply Writer'].value_counts()

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(os.path.dirname(file_path),
                                  os.path.basename(file_path).replace('.csv', '') + '_analysis')
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding='utf-8-sig')
        time_analysis.to_csv(os.path.join(
            csv_output_dir, "time_analysis.csv"), encoding='utf-8-sig')
        month_analysis.to_csv(os.path.join(
            csv_output_dir, "month_analysis.csv"), encoding='utf-8-sig', index=False)
        sentiment_counts.to_csv(os.path.join(
            csv_output_dir, "sentiment_counts.csv"), encoding='utf-8-sig')
        correlation_matrix.to_csv(os.path.join(
            csv_output_dir, "correlation_matrix.csv"), encoding='utf-8-sig')
        writer_reply_count.to_csv(os.path.join(
            csv_output_dir, "writer_reply_count.csv"), encoding='utf-8-sig')

        # ì‹œê°í™” ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥

        # 1. ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸
        data_length = len(time_analysis)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.lineplot(data=time_analysis,
                     x=time_analysis.index, y='Reply Count')
        plt.title('Daily Reply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Number of Replies')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "daily_reply_count.png"))
        plt.close()

        # For month_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(month_analysis)))
        sns.lineplot(data=month_analysis, x='Reply Date',
                     y='Reply Count', label='Reply Count')
        sns.lineplot(data=month_analysis, x='Reply Date',
                     y='Reply Like', label='Likes')
        sns.lineplot(data=month_analysis, x='Reply Date',
                     y='Reply Bad', label='Dislikes')
        plt.title('Monthly Reply Count, Likes, and Dislikes Over Time')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "monthly_reply_count.png"))
        plt.close()

        # 2. ëŒ“ê¸€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë¶„í¬
        data_length = len(sentiment_counts)
        plt.figure(figsize=self.calculate_figsize(data_length, base_width=8))
        sns.countplot(x='Reply Sentiment', data=data)
        plt.title('Reply Sentiment Distribution')
        plt.xlabel('Sentiment')
        plt.ylabel('Count')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "reply_sentiment_distribution.png"))
        plt.close()

        # 4. ìƒê´€ê´€ê³„ í–‰ë ¬ íˆíŠ¸ë§µ
        data_length = len(correlation_matrix)
        plt.figure(figsize=self.calculate_figsize(data_length, height=8))
        sns.heatmap(correlation_matrix, annot=True,
                    cmap='coolwarm', vmin=-1, vmax=1)
        plt.title('Correlation Matrix of Key Metrics')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "correlation_matrix.png"))
        plt.close()

        # 5. ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…)
        top_10_writers = writer_reply_count.head(10)  # ìƒìœ„ 10ëª… ì‘ì„±ì ì„ íƒ
        data_length = len(top_10_writers)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.barplot(x=top_10_writers.index,
                    y=top_10_writers.values, palette="viridis")
        plt.title('Top 10 Writers by Number of Replies')
        plt.xlabel('Writer')
        plt.ylabel('Number of Replies')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "writer_reply_count.png"))
        plt.close()

        top_n = 10
        top_writers = writer_reply_count.sort_values(
            ascending=False).head(top_n).index

        filtered_reply_dir = os.path.join(csv_output_dir, 'user_replies')
        os.makedirs(filtered_reply_dir)
        # ê° ìƒìœ„ ì‘ì„±ìì˜ ëŒ“ê¸€ì„ ë³„ë„ CSV íŒŒì¼ë¡œ ì €ì¥
        for index, writer in enumerate(top_writers):
            writer_data = data[data['Reply Writer'] == writer]
            writer_csv_path = os.path.join(
                filtered_reply_dir, f"{index+1}_{writer}_replies.csv").replace('*', '')
            writer_data.to_csv(
                writer_csv_path, encoding='utf-8-sig', index=False)

        # ê·¸ë˜í”„ ì„¤ëª… ì‘ì„± (í•œêµ­ì–´)
        description_text = """
        ê·¸ë˜í”„ ì„¤ëª…:

        1. ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸ (daily_reply_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ íŠ¹ì • ê¸°ê°„ ë™ì•ˆ ëŒ“ê¸€ì´ ì–¼ë§ˆë‚˜ ë§ì´ ë‹¬ë ¸ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        2. ëŒ“ê¸€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë¶„í¬ (reply_sentiment_distribution.png):
           - ì´ ê·¸ë˜í”„ëŠ” ëŒ“ê¸€ì˜ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤.
           - xì¶•ì€ ê°ì„±ì˜ ìœ í˜•(ê¸ì •, ë¶€ì •, ì¤‘ë¦½)ì„, yì¶•ì€ í•´ë‹¹ ê°ì„±ì˜ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ëŒ“ê¸€ì˜ ì „ë°˜ì ì¸ ê°ì„± ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        3. ìƒê´€ê´€ê³„ í–‰ë ¬ íˆíŠ¸ë§µ (correlation_matrix.png):
           - ì´ íˆíŠ¸ë§µì€ ì£¼ìš” ì§€í‘œë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤.
           - ìƒ‰ìƒì´ ì§„í• ìˆ˜ë¡ ìƒê´€ê´€ê³„ê°€ ë†’ìŒì„ ë‚˜íƒ€ë‚´ë©°, ìŒìˆ˜ëŠ” ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        4. ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…) (writer_reply_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ëŒ“ê¸€ì„ ê°€ì¥ ë§ì´ ì‘ì„±í•œ ìƒìœ„ 10ëª…ì˜ ì‘ì„±ìë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ì‘ì„±ìì˜ ì´ë¦„ì„, yì¶•ì€ í•´ë‹¹ ì‘ì„±ìê°€ ì‘ì„±í•œ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ ì–´ë–¤ ì‘ì„±ìê°€ ëŒ“ê¸€ í™œë™ì´ í™œë°œí•œì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
           
        5. ì›”ë³„ ëŒ“ê¸€ í†µê³„ ë¶„ì„ (monthly_reply_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ì›”ë³„ ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜, ì‹«ì–´ìš” ìˆ˜ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ ìˆ˜ëŸ‰ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ íŠ¹ì • ì›”ì— ëŒ“ê¸€ í™œë™ì´ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•œ íŒ¨í„´ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        
        if all(col in data.columns for col in ['TotalUserComment', 'TotalUserReply', 'TotalUserLike']):
            # 1. ì‚¬ìš©ìë³„ ì´í•© ì§‘ê³„
            user_activity = data.groupby('Reply Writer').agg({
                'TotalUserComment': 'max',
                'TotalUserReply': 'max',
                'TotalUserLike': 'max'
            }).sort_values(by='TotalUserComment', ascending=False)

            # ê²°ê³¼ ì €ì¥
            user_activity.to_csv(os.path.join(csv_output_dir, "user_activity.csv"), encoding='utf-8-sig')

            # 2. Top 10 ì‚¬ìš©ì ê·¸ë˜í”„ (ëŒ“ê¸€ ìˆ˜, ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜)
            top_user_activity = user_activity.head(10)

            # ì´ ëŒ“ê¸€ ìˆ˜
            plt.figure(figsize=self.calculate_figsize(len(top_user_activity)))
            sns.barplot(x=top_user_activity.index, y=top_user_activity['TotalUserComment'], palette='Blues_r')
            plt.title('Top 10 Users by Total Comments')
            plt.xlabel('User')
            plt.ylabel('Total Comments')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "top_users_total_comments.png"))
            plt.close()

            # ì´ ëŒ€ëŒ“ê¸€ ìˆ˜
            top_user_reply = top_user_activity.sort_values(by='TotalUserReply', ascending=False)
            plt.figure(figsize=self.calculate_figsize(len(top_user_reply)))
            sns.barplot(x=top_user_reply.index, y=top_user_reply['TotalUserReply'], palette='Greens_r')
            plt.title('Top 10 Users by Total Replies')
            plt.xlabel('User')
            plt.ylabel('Total Replies')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "top_users_total_replies.png"))
            plt.close()

            # ì´ ì¢‹ì•„ìš” ìˆ˜
            top_user_like = top_user_activity.sort_values(by='TotalUserLike', ascending=False)
            plt.figure(figsize=self.calculate_figsize(len(top_user_like)))
            sns.barplot(x=top_user_like.index, y=top_user_like['TotalUserLike'], palette='Oranges_r')
            plt.title('Top 10 Users by Total Likes')
            plt.xlabel('User')
            plt.ylabel('Total Likes')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "top_users_total_likes.png"))
            plt.close()

            # ------------------- 3. ì‚¬ìš©ì í™œë™ëŸ‰ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨) -------------------
            plt.figure(figsize=(10, 6))
            sns.histplot(user_activity['TotalUserComment'], bins=30, kde=True)
            plt.title('Distribution of Total Comments per User')
            plt.xlabel('Total Comments')
            plt.ylabel('User Count')
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "user_comment_distribution.png"))
            plt.close()

            plt.figure(figsize=(10, 6))
            sns.histplot(user_activity['TotalUserReply'], bins=30, kde=True, color='green')
            plt.title('Distribution of Total Replies per User')
            plt.xlabel('Total Replies')
            plt.ylabel('User Count')
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "user_reply_distribution.png"))
            plt.close()

            plt.figure(figsize=(10, 6))
            sns.histplot(user_activity['TotalUserLike'], bins=30, kde=True, color='orange')
            plt.title('Distribution of Total Likes per User')
            plt.xlabel('Total Likes')
            plt.ylabel('User Count')
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "user_like_distribution.png"))
            plt.close()

            # ------------------- 4. ì‚¬ìš©ì í™œë™ëŸ‰ ìƒê´€ê´€ê³„ ë¶„ì„ -------------------
            corr_user = user_activity.corr()

            plt.figure(figsize=(6, 5))
            sns.heatmap(corr_user, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
            plt.title('Correlation between User Activity Metrics')
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "user_activity_correlation.png"))
            plt.close()

            corr_user.to_csv(os.path.join(csv_output_dir, "user_activity_correlation.csv"), encoding='utf-8-sig')

            # ------------------- 5. í™œë™ ìƒìœ„ 10% ì‚¬ìš©ì íŒŒì•… -------------------
            top_10_percent_threshold = user_activity['TotalUserComment'].quantile(0.9)
            top_active_users = user_activity[user_activity['TotalUserComment'] >= top_10_percent_threshold]
            top_active_users.to_csv(os.path.join(csv_output_dir, "top_10_percent_users.csv"), encoding='utf-8-sig')

            # ------------------- 6. ì‚¬ìš©ì í™œë™ ì§€ìˆ˜ (ê°€ì¤‘ì¹˜ ì§€í‘œ) -------------------
            # ì˜ˆ: ëŒ“ê¸€ 1ì , ëŒ€ëŒ“ê¸€ 1.5ì , ì¢‹ì•„ìš” 0.5ì 
            user_activity['ActivityScore'] = (
                user_activity['TotalUserComment'] * 1.0 +
                user_activity['TotalUserReply'] * 1.5 +
                user_activity['TotalUserLike'] * 0.5
            )

            user_activity_sorted = user_activity.sort_values(by='ActivityScore', ascending=False)
            top_user_score = user_activity_sorted.head(10)

            plt.figure(figsize=self.calculate_figsize(len(top_user_score)))
            sns.barplot(x=top_user_score.index, y=top_user_score['ActivityScore'], palette='Purples_r')
            plt.title('Top 10 Users by Activity Score')
            plt.xlabel('User')
            plt.ylabel('Activity Score')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(graph_output_dir, "top_users_activity_score.png"))
            plt.close()

            user_activity_sorted.to_csv(os.path.join(csv_output_dir, "user_activity_with_score.csv"), encoding='utf-8-sig')

            # ì„¤ëª… í…ìŠ¤íŠ¸ì— ì¶”ê°€
            description_text += """

        6. ì‚¬ìš©ì í™œë™ í†µê³„ ë¶„ì„ (user_activity ë¶„ì„ ê²°ê³¼):
            - ì´ ë¶„ì„ì€ ê° ì‚¬ìš©ìì˜ ì „ì²´ í™œë™ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ“ê¸€ ìˆ˜, ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ë¥¼ ì§‘ê³„í•˜ê³  
                ì‚¬ìš©ì ê°„ì˜ í™œë™ íŒ¨í„´ì„ ë¹„êµí•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
            - 'TotalUserComment', 'TotalUserReply', 'TotalUserLike' ì—´ì´ ì¡´ì¬í•  ê²½ìš°ì—ë§Œ ì‹¤í–‰ë©ë‹ˆë‹¤.

            1) user_activity.csv:
                - ê° ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì´ ëŒ“ê¸€ ìˆ˜, ì´ ëŒ€ëŒ“ê¸€ ìˆ˜, ì´ ì¢‹ì•„ìš” ìˆ˜ë¥¼ ì§‘ê³„í•œ íŒŒì¼ì…ë‹ˆë‹¤.
                - ë™ì¼ ì‘ì„±ìì— ëŒ€í•´ ì—¬ëŸ¬ ëŒ“ê¸€ì´ ì¡´ì¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ì¥ í° ëˆ„ì ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë¦¬ë©ë‹ˆë‹¤.
                - ì´ë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ì „ì²´ í™œë™ ê·œëª¨ë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            2) top_users_total_comments.png / top_users_total_replies.png / top_users_total_likes.png:
                - ê°ê° ëŒ“ê¸€ ìˆ˜, ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 10ëª…ì˜ ì‚¬ìš©ìë¥¼ ì‹œê°í™”í•œ ë§‰ëŒ€ ê·¸ë˜í”„ì…ë‹ˆë‹¤.
                - ëŒ“ê¸€ ì¤‘ì‹¬ í™œë™ì, ëŒ€ëŒ“ê¸€ ì¤‘ì‹¬ í™œë™ì, ì¢‹ì•„ìš”ë¥¼ ë§ì´ ë°›ì€ ì‚¬ìš©ìë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - ê·¸ë˜í”„ë¥¼ í†µí•´ í™œë™ íŒ¨í„´ì˜ ë¶ˆê· í˜•(ì†Œìˆ˜ì˜ í™œë™ ì§‘ì¤‘ í˜„ìƒ ë“±)ë„ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.

            3) user_comment_distribution.png / user_reply_distribution.png / user_like_distribution.png:
                - ì‚¬ìš©ì ì „ì²´ì˜ í™œë™ëŸ‰ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ” íˆìŠ¤í† ê·¸ë¨ì…ë‹ˆë‹¤.
                - ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš©ìê°€ ë‚®ì€ í™œë™ëŸ‰ì„ ë³´ì´ëŠ” â€˜ê¸´ ê¼¬ë¦¬(long tail)â€™ í˜„ìƒì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - KDE(í™•ë¥ ë°€ë„ê³¡ì„ )ê°€ í•¨ê»˜ í‘œì‹œë˜ì–´ í‰ê· ì ì¸ í™œë™ ìˆ˜ì¤€ê³¼ ë¶„í¬ì˜ ì¹˜ìš°ì¹¨ ì •ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            4) user_activity_correlation.png:
                - ëŒ“ê¸€ ìˆ˜, ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ì£¼ëŠ” íˆíŠ¸ë§µì…ë‹ˆë‹¤.
                - ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ë†’ì„ ê²½ìš°, ëŒ“ê¸€ì„ ë§ì´ ì‘ì„±í•œ ì‚¬ìš©ìê°€ ëŒ€ëŒ“ê¸€ê³¼ ì¢‹ì•„ìš”ë„ ë§ì´ ë°›ëŠ” ê²½í–¥ì´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
                - ìŒì˜ ìƒê´€ê´€ê³„ê°€ ìˆì„ ê²½ìš°, íŠ¹ì • í™œë™ ìœ í˜•(ì˜ˆ: ëŒ“ê¸€)ê³¼ ë‹¤ë¥¸ í™œë™(ì˜ˆ: ì¢‹ì•„ìš” ìˆ˜ì§‘)ì´ ë°˜ë¹„ë¡€í•  ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

            5) top_10_percent_users.csv:
                - ì „ì²´ ì‚¬ìš©ì ì¤‘ ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10%ì— í•´ë‹¹í•˜ëŠ” í™œë™ì ì¸ ì‚¬ìš©ì ëª©ë¡ì…ë‹ˆë‹¤.
                - í™œë°œí•œ ì‚¬ìš©ì ê·¸ë£¹ì„ ë³„ë„ë¡œ ë¶„ì„í•˜ê±°ë‚˜, ì˜í–¥ë ¥ ìˆëŠ” ì‚¬ìš©ìêµ°ì„ íŒŒì•…í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.

            6) user_activity_with_score.csv / top_users_activity_score.png:
                - ëŒ“ê¸€, ëŒ€ëŒ“ê¸€, ì¢‹ì•„ìš” ê°ê°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ ì‚°ì¶œí•œ â€˜í™œë™ ì§€ìˆ˜(Activity Score)â€™ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë ¬í•œ ê²°ê³¼ì…ë‹ˆë‹¤.
                    (ê¸°ë³¸ ê°€ì¤‘ì¹˜: ëŒ“ê¸€Ã—1.0 + ëŒ€ëŒ“ê¸€Ã—1.5 + ì¢‹ì•„ìš”Ã—0.5)
                - í™œë™ ì§€ìˆ˜ëŠ” ë‹¨ìˆœ í™œë™ëŸ‰ë³´ë‹¤ â€œì°¸ì—¬ì˜ ì§ˆì  ìˆ˜ì¤€â€ì„ ë°˜ì˜í•˜ë©°, 
                    í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬ìë‚˜ ì¸í”Œë£¨ì–¸ì„œí˜• ì‚¬ìš©ìë¥¼ ì„ ë³„í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
                - top_users_activity_score.pngëŠ” í™œë™ ì§€ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 10ëª…ì˜ ì‚¬ìš©ìë¥¼ ì‹œê°í™”í•œ ê·¸ë˜í”„ì…ë‹ˆë‹¤.

            ğŸ” ë¶„ì„ í™œìš© ì˜ˆì‹œ:
                - ëŒ“ê¸€ ìˆ˜ ëŒ€ë¹„ ì¢‹ì•„ìš” ìˆ˜ì˜ ë¹„ìœ¨ì´ ë†’ì€ ì‚¬ìš©ìë¥¼ í†µí•´ ì˜í–¥ë ¥ ìˆëŠ” ì˜ê²¬ ë¦¬ë”ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - í™œë™ëŸ‰ì´ ë§ìœ¼ë‚˜ ì¢‹ì•„ìš” ìˆ˜ê°€ ì ì€ ì‚¬ìš©ìëŠ” ë…¼ìŸì ì´ê±°ë‚˜ ë¹„íŒì ì¸ ì„±í–¥ì„ ë³´ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.
                - ìƒìœ„ 10% ì‚¬ìš©ìêµ°ì˜ í™œë™ ì‹œì ê³¼ ê°ì„± ë¶„í¬ë¥¼ ê²°í•© ë¶„ì„í•˜ë©´ ì»¤ë®¤ë‹ˆí‹°ì˜ ì£¼ìš” íŠ¸ë Œë“œë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            """

        # ì„¤ëª…ì„ txt íŒŒì¼ë¡œ ì €ì¥
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def NaverNewsRereplyAnalysis(self, data, file_path):
        if not self.checkColumns([
            "Reply_ID", 
            "Rereply Writer", 
            "Rereply Date", 
            "Rereply Text", 
            "Rereply Like",
            "Rereply Bad", 
            "Rereply LikeRatio", 
            "Rereply Sentiment", 
            "Article URL", 
            'Article Day'
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 'Rereply Date'ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì˜¤ë¥˜ ë°œìƒ ì‹œ NaTë¡œ ë³€í™˜)
        data['Rereply Date'] = pd.to_datetime(
            data['Rereply Date'], errors='coerce')

        # ìˆ«ìí˜• ì»¬ëŸ¼ì„ ìˆ«ì(float)ë¡œ ë³€í™˜, ë³€í™˜ ë¶ˆê°€ ì‹œ 0ìœ¼ë¡œ ì±„ì›€
        for col in ['Rereply Like', 'Rereply Bad', 'Rereply LikeRatio', 'Rereply Sentiment']:
            data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)

        # 'Rereply Text'ê°€ ê²°ì¸¡ê°’ì´ ì•„ë‹Œì§€ í™•ì¸í•˜ê³  ê¸¸ì´ë¥¼ ê³„ì‚°
        data['Rereply Text'] = data['Rereply Text'].fillna('')
        data['Rereply Length'] = data['Rereply Text'].apply(len)

        # ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ ë¶„ì„
        time_analysis = data.groupby(data['Rereply Date'].dt.date).agg({
            'id': 'count',
            'Rereply Like': 'sum',
            'Rereply Bad': 'sum'
        }).rename(columns={'id': 'Rereply Count'}).reset_index()

        # ì›”ë³„ ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš”, ì‹«ì–´ìš” í•©ê³„ ë¶„ì„
        month_analysis = data.groupby(data['Rereply Date'].dt.to_period("M")).agg({
            'id': 'count',
            'Rereply Like': 'sum',
            'Rereply Bad': 'sum'
        }).rename(columns={'id': 'Rereply Count'}).reset_index()
        month_analysis['Rereply Date'] = month_analysis['Rereply Date'].dt.to_timestamp()

        # ëŒ“ê¸€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë¹ˆë„
        sentiment_counts = data['Rereply Sentiment'].value_counts()

        # ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ)
        numeric_columns = ['Rereply Like', 'Rereply Bad',
                           'Rereply Length', 'Rereply LikeRatio', 'Rereply Sentiment']
        correlation_matrix = data[numeric_columns].corr()

        # ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ê³„ì‚°
        writer_reply_count = data['Rereply Writer'].value_counts()

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(os.path.dirname(file_path),
                                  os.path.basename(file_path).replace('.csv', '') + '_analysis')
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        basic_stats = data.describe(include='all')
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding='utf-8-sig')
        time_analysis.to_csv(os.path.join(
            csv_output_dir, "time_analysis.csv"), encoding='utf-8-sig', index=False)
        month_analysis.to_csv(os.path.join(
            csv_output_dir, "month_analysis.csv"), encoding='utf-8-sig', index=False)
        sentiment_counts.to_csv(os.path.join(
            csv_output_dir, "sentiment_counts.csv"), encoding='utf-8-sig')
        correlation_matrix.to_csv(os.path.join(
            csv_output_dir, "correlation_matrix.csv"), encoding='utf-8-sig')
        writer_reply_count.to_csv(os.path.join(
            csv_output_dir, "writer_rereply_count.csv"), encoding='utf-8-sig')

        # ì‹œê°í™” ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥

        # 1. ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸
        plt.figure(figsize=self.calculate_figsize(len(time_analysis)))
        sns.lineplot(data=time_analysis, x='Rereply Date', y='Rereply Count')
        plt.title('Daily Rereply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Number of Rereplies')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "daily_rereply_count.png"))
        plt.close()

        # For month_analysis graph
        plt.figure(figsize=self.calculate_figsize(len(month_analysis)))
        sns.lineplot(data=month_analysis, x='Rereply Date',
                     y='Rereply Count', label='Rereply Count')
        sns.lineplot(data=month_analysis, x='Rereply Date',
                     y='Rereply Like', label='Likes')
        sns.lineplot(data=month_analysis, x='Rereply Date',
                     y='Rereply Bad', label='Dislikes')
        plt.title('Monthly Rereply Count, Likes, and Dislikes Over Time')
        plt.xlabel('Date')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "monthly_reply_count.png"))
        plt.close()

        # 2. ëŒ“ê¸€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë¶„í¬
        data_length = len(sentiment_counts)
        plt.figure(figsize=self.calculate_figsize(data_length, base_width=8))
        sns.countplot(x='Rereply Sentiment', data=data.fillna(''))
        plt.title('Rereply Sentiment Distribution')
        plt.xlabel('Sentiment')
        plt.ylabel('Count')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "rereply_sentiment_distribution.png"))
        plt.close()

        # 4. ìƒê´€ê´€ê³„ í–‰ë ¬ íˆíŠ¸ë§µ
        data_length = len(correlation_matrix)
        plt.figure(figsize=self.calculate_figsize(data_length, height=8))
        sns.heatmap(correlation_matrix, annot=True,
                    cmap='coolwarm', vmin=-1, vmax=1)
        plt.title('Correlation Matrix of Key Metrics')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "correlation_matrix.png"))
        plt.close()

        # 5. ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…)
        top_10_writers = writer_reply_count.head(10)
        plt.figure(figsize=self.calculate_figsize(len(top_10_writers)))
        sns.barplot(x=top_10_writers.index,
                    y=top_10_writers.values, palette="viridis")
        plt.title('Top 10 Writers by Number of Rereplies')
        plt.xlabel('Writer')
        plt.ylabel('Number of Rereplies')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "writer_rereply_count.png"))
        plt.close()

        # ê·¸ë˜í”„ ì„¤ëª… ì‘ì„± (í•œêµ­ì–´)
        description_text = """
            ê·¸ë˜í”„ ì„¤ëª…:

            1. ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸ (daily_rereply_count.png):
               - ì´ ê·¸ë˜í”„ëŠ” ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
               - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
               - ì´ë¥¼ í†µí•´ íŠ¹ì • ê¸°ê°„ ë™ì•ˆ ëŒ“ê¸€ì´ ì–¼ë§ˆë‚˜ ë§ì´ ë‹¬ë ¸ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            2. ëŒ“ê¸€ ê°ì„± ë¶„ì„ ê²°ê³¼ ë¶„í¬ (rereply_sentiment_distribution.png):
               - ì´ ê·¸ë˜í”„ëŠ” ëŒ“ê¸€ì˜ ê°ì„± ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤.
               - xì¶•ì€ ê°ì„±ì˜ ìœ í˜•(ê¸ì •, ë¶€ì •, ì¤‘ë¦½)ì„, yì¶•ì€ í•´ë‹¹ ê°ì„±ì˜ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
               - ëŒ“ê¸€ì˜ ì „ë°˜ì ì¸ ê°ì„± ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            3. ìƒê´€ê´€ê³„ í–‰ë ¬ íˆíŠ¸ë§µ (correlation_matrix.png):
               - ì´ íˆíŠ¸ë§µì€ ì£¼ìš” ì§€í‘œë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•œ ê²ƒì…ë‹ˆë‹¤.
               - ìƒ‰ìƒì´ ì§„í• ìˆ˜ë¡ ìƒê´€ê´€ê³„ê°€ ë†’ìŒì„ ë‚˜íƒ€ë‚´ë©°, ìŒìˆ˜ëŠ” ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
               - ì´ë¥¼ í†µí•´ ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            4. ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…) (writer_rereply_count.png):
               - ì´ ê·¸ë˜í”„ëŠ” ëŒ“ê¸€ì„ ê°€ì¥ ë§ì´ ì‘ì„±í•œ ìƒìœ„ 10ëª…ì˜ ì‘ì„±ìë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
               - xì¶•ì€ ì‘ì„±ìì˜ ì´ë¦„ì„, yì¶•ì€ í•´ë‹¹ ì‘ì„±ìê°€ ì‘ì„±í•œ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
               - ì´ë¥¼ í†µí•´ ì–´ë–¤ ì‘ì„±ìê°€ ëŒ“ê¸€ í™œë™ì´ í™œë°œí•œì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        # ì„¤ëª…ì„ txt íŒŒì¼ë¡œ ì €ì¥
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def NaverCafeArticleAnalysis(self, data, file_path):
        if not self.checkColumns([
            "NaverCafe Name", 
            "NaverCafe MemberCount", 
            "Article Writer", 
            "Article Title",
            "Article Text", 
            "Article Date", 
            "Article ReadCount", 
            "Article ReplyCount", 
            "Article URL"
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 'Article Date'ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        data['Article Date'] = pd.to_datetime(data['Article Date'])
        for col in ['NaverCafe MemberCount', 'Article ReadCount', 'Article ReplyCount']:
            data[col] = pd.to_numeric(
                data[col], errors='coerce')  # ê° ì—´ì„ ìˆ«ìë¡œ ë³€í™˜

        # ê¸°ë³¸ í†µê³„ ë¶„ì„
        basic_stats = data.describe(include='all')

        # ì¹´í˜ë³„ ë¶„ì„
        cafe_analysis = data.groupby('NaverCafe Name').agg({
            'id': 'count',
            'Article ReadCount': 'mean',
            'Article ReplyCount': 'mean',
            'NaverCafe MemberCount': 'mean'
        }).rename(columns={'id': 'Article Count', 'Article ReadCount': 'Avg ReadCount',
                           'Article ReplyCount': 'Avg ReplyCount'})

        # ì‘ì„±ìë³„ ë¶„ì„
        writer_analysis = data.groupby('Article Writer').agg({
            'id': 'count',
            'Article ReadCount': 'mean',
            'Article ReplyCount': 'mean'
        }).rename(columns={'id': 'Article Count', 'Article ReadCount': 'Avg ReadCount',
                           'Article ReplyCount': 'Avg ReplyCount'})

        # ì‹œê°„ë³„ ë¶„ì„ (ì—°ë„, ì›”ë³„)
        time_analysis = data.groupby(data['Article Date'].dt.to_period("M")).agg({
            'id': 'count',
            'Article ReadCount': 'sum',
            'Article ReplyCount': 'sum'
        }).rename(columns={'id': 'Article Count'})

        # ìƒê´€ê´€ê³„ ë¶„ì„
        numerical_cols = ['NaverCafe MemberCount',
                          'Article ReadCount', 'Article ReplyCount']
        correlation_matrix = data[numerical_cols].corr()

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(os.path.dirname(file_path),
                                  os.path.basename(file_path).replace('.csv', '') + '_analysis')
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding='utf-8-sig')
        cafe_analysis.to_csv(os.path.join(
            csv_output_dir, "cafe_analysis.csv"), encoding='utf-8-sig')
        writer_analysis.to_csv(os.path.join(
            csv_output_dir, "writer_analysis.csv"), encoding='utf-8-sig')
        time_analysis.to_csv(os.path.join(
            csv_output_dir, "time_analysis.csv"), encoding='utf-8-sig')
        correlation_matrix.to_csv(os.path.join(
            csv_output_dir, "correlation_matrix.csv"), encoding='utf-8-sig')

        # ì‹œê°í™” ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥

        # 1. ì¹´í˜ë³„ ê²Œì‹œê¸€ ìˆ˜ ë¶„í¬
        data_length = len(cafe_analysis)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.barplot(x=cafe_analysis.index, y=cafe_analysis['Article Count'])
        plt.title('Number of Articles by NaverCafe')
        plt.xlabel('NaverCafe')
        plt.ylabel('Number of Articles')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "cafe_article_count.png"))
        plt.close()

        # 2. ì‹œê°„ë³„ ê²Œì‹œê¸€ ìˆ˜ ì¶”ì„¸
        data_length = len(time_analysis)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.lineplot(data=time_analysis,
                     x=time_analysis.index.to_timestamp(), y='Article Count')
        plt.title('Monthly Article Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Number of Articles')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "monthly_article_count.png"))
        plt.close()

        # 4. ì‘ì„±ìë³„ ê²Œì‹œê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…)
        top_10_writers = writer_analysis.sort_values(
            'Article Count', ascending=False).head(10)
        data_length = len(top_10_writers)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.barplot(x=top_10_writers.index, y=top_10_writers['Article Count'])
        plt.title('Top 10 Writers by Number of Articles')
        plt.xlabel('Writer')
        plt.ylabel('Number of Articles')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_10_writers.png"))
        plt.close()

        # ê·¸ë˜í”„ ì„¤ëª… ì‘ì„± (í•œêµ­ì–´)
        description_text = """
        ê·¸ë˜í”„ ì„¤ëª…:

        1. ì¹´í˜ë³„ ê²Œì‹œê¸€ ìˆ˜ ë¶„í¬ (cafe_article_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ê° ë„¤ì´ë²„ ì¹´í˜ë³„ë¡œ ì‘ì„±ëœ ê²Œì‹œê¸€ ìˆ˜ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë„¤ì´ë²„ ì¹´í˜ëª…ì„, yì¶•ì€ í•´ë‹¹ ì¹´í˜ì—ì„œ ì‘ì„±ëœ ê²Œì‹œê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ ê° ì¹´í˜ì—ì„œì˜ ê²Œì‹œê¸€ ì‘ì„± í™œë™ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        2. ì‹œê°„ë³„ ê²Œì‹œê¸€ ìˆ˜ ì¶”ì„¸ (monthly_article_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ì‹œê°„ì— ë”°ë¥¸ ì›”ë³„ ê²Œì‹œê¸€ ìˆ˜ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ í•´ë‹¹ ì›”ì— ì‘ì„±ëœ ê²Œì‹œê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ íŠ¹ì • ê¸°ê°„ ë™ì•ˆì˜ ê²Œì‹œê¸€ ì‘ì„± ì¶”ì„¸ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        3. ì‘ì„±ìë³„ ê²Œì‹œê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…) (top_10_writers.png):
           - ì´ ê·¸ë˜í”„ëŠ” ê²Œì‹œê¸€ì„ ê°€ì¥ ë§ì´ ì‘ì„±í•œ ìƒìœ„ 10ëª…ì˜ ì‘ì„±ìë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ì‘ì„±ìëª…ì„, yì¶•ì€ í•´ë‹¹ ì‘ì„±ìê°€ ì‘ì„±í•œ ê²Œì‹œê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ ì–´ë–¤ ì‘ì„±ìê°€ ê²Œì‹œê¸€ í™œë™ì´ í™œë°œí•œì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """

        # ì„¤ëª…ì„ txt íŒŒì¼ë¡œ ì €ì¥
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def NaverCafeReplyAnalysis(self, data, file_path):
        if not self.checkColumns([
            "Reply Num", 
            "Reply Writer", 
            "Reply Date",
            'Reply Text', 
            'Article URL', 
            'Article Day'
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 'Reply Date'ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        data['Reply Date'] = pd.to_datetime(data['Reply Date'])

        # ì‘ì„±ìë³„ ë¶„ì„ (ìƒìœ„ 10ëª…)
        writer_analysis = data.groupby('Reply Writer').agg({
            'id': 'count'
        }).rename(columns={'id': 'Reply Count'}).sort_values(by='Reply Count', ascending=False).head(100)

        # ì‹œê°„ë³„ ë¶„ì„ (ì—°ë„, ì›”ë³„)
        time_analysis = data.groupby(data['Reply Date'].dt.to_period("M")).agg({
            'id': 'count'
        }).rename(columns={'id': 'Reply Count'})

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(os.path.dirname(file_path),
                                  os.path.basename(file_path).replace('.csv', '') + '_analysis')
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        writer_analysis.to_csv(os.path.join(
            csv_output_dir, "writer_analysis.csv"), encoding='utf-8-sig')
        time_analysis.to_csv(os.path.join(
            csv_output_dir, "time_analysis.csv"), encoding='utf-8-sig')

        # ì‹œê°í™” ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥

        # 1. ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 10ëª…)
        data_length = len(writer_analysis)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.barplot(x=writer_analysis.index, y=writer_analysis['Reply Count'])
        plt.title('Number of Replies by Top 100 Writers')
        plt.xlabel('Writer')
        plt.ylabel('Number of Replies')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "writer_reply_count.png"))
        plt.close()

        # 2. ì‹œê°„ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸
        data_length = len(time_analysis)
        plt.figure(figsize=self.calculate_figsize(data_length))
        sns.lineplot(data=time_analysis,
                     x=time_analysis.index.to_timestamp(), y='Reply Count')
        plt.title('Monthly Reply Count Over Time')
        plt.xlabel('Date')
        plt.ylabel('Number of Replies')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "monthly_reply_count.png"))
        plt.close()

        # ê·¸ë˜í”„ ì„¤ëª… ì‘ì„± (í•œêµ­ì–´)
        description_text = """
        ê·¸ë˜í”„ ì„¤ëª…:

        1. ì‘ì„±ìë³„ ëŒ“ê¸€ ìˆ˜ ë¶„í¬ (ìƒìœ„ 100ëª…) (writer_reply_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ëŒ“ê¸€ì„ ê°€ì¥ ë§ì´ ì‘ì„±í•œ ìƒìœ„ 100ëª…ì˜ ì‘ì„±ìë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ì‘ì„±ìëª…ì„, yì¶•ì€ í•´ë‹¹ ì‘ì„±ìê°€ ì‘ì„±í•œ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ ì–´ë–¤ ì‘ì„±ìê°€ ëŒ“ê¸€ í™œë™ì´ í™œë°œí•œì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        2. ì‹œê°„ë³„ ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸ (monthly_reply_count.png):
           - ì´ ê·¸ë˜í”„ëŠ” ì‹œê°„ì— ë”°ë¥¸ ì›”ë³„ ëŒ“ê¸€ ìˆ˜ì˜ ë³€í™”ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
           - xì¶•ì€ ë‚ ì§œë¥¼, yì¶•ì€ í•´ë‹¹ ì›”ì— ì‘ì„±ëœ ëŒ“ê¸€ ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì´ë¥¼ í†µí•´ íŠ¹ì • ê¸°ê°„ ë™ì•ˆì˜ ëŒ“ê¸€ ì‘ì„± ì¶”ì„¸ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """

        # ì„¤ëª…ì„ txt íŒŒì¼ë¡œ ì €ì¥
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def YouTubeArticleAnalysis(self, data, file_path):
        if not self.checkColumns([
            'YouTube Channel', 
            'Article URL', 
            'Article Title', 
            'Article Text',
            'Article Date', 
            'Article ViewCount', 
            'Article Like', 
            'Article ReplyCount'
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 2) ë‚ ì§œ, ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
        data['Article Date'] = pd.to_datetime(
            data['Article Date'], errors='coerce')

        # 'Article ViewCount', 'Article Like', 'Article ReplyCount' -> ìˆ«ìí˜•ìœ¼ë¡œ
        data.rename(columns={
            'Article ViewCount': 'views',
            'Article Like': 'likes',
            'Article ReplyCount': 'comments_count'
        }, inplace=True)

        for col in ['views', 'likes', 'comments_count']:
            data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)

        # 3) ê²°ê³¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(
            os.path.dirname(file_path),
            os.path.basename(file_path).replace('.csv', '') + '_analysis'
        )
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # --------------------------------------------------------------------------------
        # 4) ê¸°ë³¸ í†µê³„
        # --------------------------------------------------------------------------------
        basic_stats = data.describe(include='all')

        # --------------------------------------------------------------------------------
        # 5) ì›”ë³„, ì¼ë³„, ì£¼ë³„ë¡œ ê·¸ë£¹í™”í•˜ê¸° ìœ„í•´ ë‚ ì§œê°€ ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
        # --------------------------------------------------------------------------------
        valid_data = data.dropna(subset=['Article Date']).copy()

        # --------------------------------------------------------------------------------
        # 5-1) ì›”ë³„ ë¶„ì„
        # --------------------------------------------------------------------------------
        monthly_data = valid_data.groupby(valid_data['Article Date'].dt.to_period("M")).agg(
            video_count=('Article Date', 'count'),
            views=('views', 'sum'),
            likes=('likes', 'sum'),
            comments_count=('comments_count', 'sum')
        ).reset_index()
        # Period -> Timestamp ë³€í™˜
        monthly_data['Article Date'] = monthly_data['Article Date'].dt.to_timestamp()

        # --------------------------------------------------------------------------------
        # 5-2) ì¼ë³„ ë¶„ì„
        # --------------------------------------------------------------------------------
        daily_data = valid_data.groupby(valid_data['Article Date'].dt.to_period("D")).agg(
            video_count=('Article Date', 'count'),
            views=('views', 'sum'),
            likes=('likes', 'sum'),
            comments_count=('comments_count', 'sum')
        ).reset_index()
        daily_data['Article Date'] = daily_data['Article Date'].dt.to_timestamp()

        # --------------------------------------------------------------------------------
        # 5-3) ì£¼ë³„ ë¶„ì„ (ë§¤ì£¼ ì¼ìš”ì¼ ê¸°ì¤€ W-SUN)
        # --------------------------------------------------------------------------------
        weekly_data = valid_data.groupby(valid_data['Article Date'].dt.to_period("W-SUN")).agg(
            video_count=('Article Date', 'count'),
            views=('views', 'sum'),
            likes=('likes', 'sum'),
            comments_count=('comments_count', 'sum')
        ).reset_index()
        weekly_data['Article Date'] = weekly_data['Article Date'].dt.to_timestamp()

        # --------------------------------------------------------------------------------
        # 6) ìš”ì¼ë³„ ë¶„ì„
        # --------------------------------------------------------------------------------
        valid_data['DayOfWeek'] = valid_data['Article Date'].dt.day_name()
        dow_analysis = valid_data.groupby('DayOfWeek').agg(
            video_count=('Article Date', 'count'),
            views=('views', 'sum'),
            likes=('likes', 'sum'),
            comments_count=('comments_count', 'sum')
        ).reset_index()

        # --------------------------------------------------------------------------------
        # 7) ì±„ë„ë³„ ë¶„ì„ (ìƒìœ„ 10ê°œ)
        # --------------------------------------------------------------------------------
        top_10_channels = data['YouTube Channel'].value_counts().head(10).index
        channel_analysis = data[data['YouTube Channel'].isin(top_10_channels)].groupby('YouTube Channel').agg(
            video_count=('Article Date', 'count'),
            total_views=('views', 'sum'),
            total_likes=('likes', 'sum'),
            total_comments=('comments_count', 'sum')
        ).reset_index()

        # --------------------------------------------------------------------------------
        # 8) ìƒìœ„ 10ê°œ ì˜ìƒ(Article Title) ë¶„ì„
        # --------------------------------------------------------------------------------
        top_10_videos = data.sort_values('views', ascending=False).head(10)[
            ['Article Title', 'YouTube Channel',
                'views', 'likes', 'comments_count']
        ].reset_index(drop=True)

        # --------------------------------------------------------------------------------
        # 9) ì¶”ê°€ ì§€í‘œ ê³„ì‚° (Like-View ë¹„ìœ¨, Comment-View ë¹„ìœ¨ ë“±)
        # --------------------------------------------------------------------------------
        data['like_view_ratio'] = data.apply(
            lambda x: x['likes'] / x['views'] if x['views'] > 0 else 0,
            axis=1
        )
        data['comment_view_ratio'] = data.apply(
            lambda x: x['comments_count'] /
            x['views'] if x['views'] > 0 else 0,
            axis=1
        )

        # --------------------------------------------------------------------------------
        # 10) ìƒê´€ê´€ê³„ ë¶„ì„ (ì¶”ê°€ ì§€í‘œ í¬í•¨)
        # --------------------------------------------------------------------------------
        numeric_columns = ['views', 'likes', 'comments_count',
                           'like_view_ratio', 'comment_view_ratio']
        correlation_matrix = data[numeric_columns].corr()

        # --------------------------------------------------------------------------------
        # 11) ë¶„ì„ ê²°ê³¼ CSV ì €ì¥
        # --------------------------------------------------------------------------------
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding='utf-8-sig')
        monthly_data.to_csv(os.path.join(
            csv_output_dir, "monthly_analysis.csv"), encoding='utf-8-sig', index=False)
        daily_data.to_csv(os.path.join(
            csv_output_dir, "daily_analysis.csv"), encoding='utf-8-sig', index=False)
        weekly_data.to_csv(os.path.join(
            csv_output_dir, "weekly_analysis.csv"), encoding='utf-8-sig', index=False)
        dow_analysis.to_csv(os.path.join(
            csv_output_dir, "day_of_week_analysis.csv"), encoding='utf-8-sig', index=False)
        channel_analysis.to_csv(os.path.join(
            csv_output_dir, "channel_analysis.csv"), encoding='utf-8-sig', index=False)
        top_10_videos.to_csv(os.path.join(
            csv_output_dir, "top_10_videos.csv"), encoding='utf-8-sig', index=False)
        correlation_matrix.to_csv(os.path.join(
            csv_output_dir, "correlation_matrix.csv"), encoding='utf-8-sig')

        # --------------------------------------------------------------------------------
        # 12) ì‹œê°í™”
        # --------------------------------------------------------------------------------

        # (1) ì›”ë³„ ì¶”ì„¸
        plt.figure(figsize=self.calculate_figsize(len(monthly_data)))
        sns.lineplot(data=monthly_data, x='Article Date',
                     y='views', label='Views')
        sns.lineplot(data=monthly_data, x='Article Date',
                     y='likes', label='Likes')
        sns.lineplot(data=monthly_data, x='Article Date',
                     y='comments_count', label='Comments')
        plt.title('ì›”ë³„ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ì¶”ì„¸')
        plt.xlabel('ì›”')
        plt.ylabel('í•©ê³„')
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "monthly_trend.png"))
        plt.close()

        # (2) ì¼ë³„ ì¶”ì„¸
        plt.figure(figsize=self.calculate_figsize(len(daily_data)))
        sns.lineplot(data=daily_data, x='Article Date',
                     y='views', label='Views')
        sns.lineplot(data=daily_data, x='Article Date',
                     y='likes', label='Likes')
        sns.lineplot(data=daily_data, x='Article Date',
                     y='comments_count', label='Comments')
        plt.title('ì¼ë³„ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ì¶”ì´')
        plt.xlabel('ì¼ì')
        plt.ylabel('í•©ê³„')
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "daily_trend.png"))
        plt.close()

        # (3) ì£¼ë³„ ì¶”ì„¸
        plt.figure(figsize=self.calculate_figsize(len(weekly_data)))
        sns.lineplot(data=weekly_data, x='Article Date',
                     y='views', label='Views')
        sns.lineplot(data=weekly_data, x='Article Date',
                     y='likes', label='Likes')
        sns.lineplot(data=weekly_data, x='Article Date',
                     y='comments_count', label='Comments')
        plt.title('ì£¼ë³„ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ì¶”ì´')
        plt.xlabel('ì£¼(ì‹œì‘ì¼ ê¸°ì¤€)')
        plt.ylabel('í•©ê³„')
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "weekly_trend.png"))
        plt.close()

        # (4) ìš”ì¼ë³„ ë¶„ì„
        plt.figure(figsize=self.calculate_figsize(len(dow_analysis)))
        dow_order = ["Monday", "Tuesday", "Wednesday",
                     "Thursday", "Friday", "Saturday", "Sunday"]
        dow_analysis['DayOfWeek'] = pd.Categorical(
            dow_analysis['DayOfWeek'], categories=dow_order, ordered=True)
        dow_analysis_sorted = dow_analysis.sort_values('DayOfWeek')
        sns.barplot(data=dow_analysis_sorted, x='DayOfWeek', y='views')
        plt.title('ìš”ì¼ë³„ ì´ ì¡°íšŒìˆ˜')
        plt.xlabel('ìš”ì¼')
        plt.ylabel('ì¡°íšŒìˆ˜')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "day_of_week_views.png"))
        plt.close()

        # (5) ìƒìœ„ 10ê°œ ì±„ë„(ì¡°íšŒìˆ˜ ê¸°ì¤€)
        plt.figure(figsize=self.calculate_figsize(len(channel_analysis)))
        channel_analysis_sorted = channel_analysis.sort_values(
            'total_views', ascending=False)
        sns.barplot(data=channel_analysis_sorted,
                    x='YouTube Channel', y='total_views')
        plt.title('ìƒìœ„ 10ê°œ ì±„ë„ë³„ ì´ ì¡°íšŒìˆ˜')
        plt.xlabel('ì±„ë„ëª…')
        plt.ylabel('ì¡°íšŒìˆ˜')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_channels_views.png"))
        plt.close()

        # (6) ìƒìœ„ 10ê°œ ì˜ìƒ(ì¡°íšŒìˆ˜ ê¸°ì¤€)
        plt.figure(figsize=self.calculate_figsize(len(top_10_videos)))
        sns.barplot(data=top_10_videos, x='Article Title', y='views')
        plt.title('ìƒìœ„ 10ê°œ ì˜ìƒ (ì¡°íšŒìˆ˜ ê¸°ì¤€)')
        plt.xlabel('ì˜ìƒ ì œëª©')
        plt.ylabel('ì¡°íšŒìˆ˜')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_10_videos.png"))
        plt.close()

        # (7) íˆìŠ¤í† ê·¸ë¨ (Like-View ë¹„ìœ¨, Comment-View ë¹„ìœ¨ ë¶„í¬)
        #     - ê·¹ë‹¨ê°’ ì œê±°(ìƒìœ„ 1%)ë¥¼ ìœ„í•´ quantile(0.99)ì„ ì‚¬ìš©í•´ xì¶• ì œí•œ
        #     - í•„ìš” ì‹œ ë¡œê·¸ ìŠ¤ì¼€ì¼(ax.set_xscale('log'))ë„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ.

        # Like-View Ratio
        plt.figure(figsize=self.calculate_figsize(10))
        ax1 = sns.histplot(data=data, x='like_view_ratio', kde=True)
        like_99 = data['like_view_ratio'].quantile(0.99)
        ax1.set_xlim(0, like_99)  # xì¶• ë²”ìœ„ë¥¼ 0~ìƒìœ„ 1% ë¶„ìœ„ìˆ˜ê¹Œì§€ë§Œ
        # ax1.set_xscale('log')   # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì˜ˆì‹œ(ì£¼ì„ í•´ì œ ì‹œ ì‚¬ìš© ê°€ëŠ¥)

        plt.title('Like-View Ratio Distribution')
        plt.xlabel('Like / View ë¹„ìœ¨')
        plt.ylabel('ë¹ˆë„')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "like_view_ratio_distribution.png"))
        plt.close()

        # Comment-View Ratio
        plt.figure(figsize=self.calculate_figsize(10))
        ax2 = sns.histplot(data=data, x='comment_view_ratio', kde=True)
        comment_99 = data['comment_view_ratio'].quantile(0.99)
        ax2.set_xlim(0, comment_99)
        # ax2.set_xscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì˜ˆì‹œ

        plt.title('Comment-View Ratio Distribution')
        plt.xlabel('Comment / View ë¹„ìœ¨')
        plt.ylabel('ë¹ˆë„')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "comment_view_ratio_distribution.png"))
        plt.close()

        # (8) ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.figure(figsize=self.calculate_figsize(
            len(correlation_matrix), height=8))
        sns.heatmap(correlation_matrix, annot=True,
                    cmap='coolwarm', vmin=-1, vmax=1)
        plt.title('ìˆ«ìí˜• ì§€í‘œ ìƒê´€ê´€ê³„ (ì¶”ê°€ ì§€í‘œ í¬í•¨)')
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "correlation_matrix.png"))
        plt.close()

        # --------------------------------------------------------------------------------
        # 13) ê·¸ë˜í”„ ì„¤ëª… í…ìŠ¤íŠ¸ ì‘ì„±
        # --------------------------------------------------------------------------------
        description_text = """
        ê·¸ë˜í”„/ë¶„ì„ ê²°ê³¼ ì„¤ëª…:

        1. ê¸°ë³¸ í†µê³„ (basic_stats.csv):
           - ë°ì´í„° ì „ì²´ì— ëŒ€í•œ ê¸°ì´ˆ í†µê³„ëŸ‰ì„ ì œê³µí•©ë‹ˆë‹¤.

        2. ì›”ë³„ íŠ¸ë Œë“œ (monthly_trend.png, monthly_analysis.csv):
           - ì›”ë³„ ì¡°íšŒìˆ˜(views), ì¢‹ì•„ìš”(likes), ëŒ“ê¸€ ìˆ˜(comments_count)ë¥¼ í•©ì‚°í•˜ì—¬ ì„  ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
           - í•´ë‹¹ ê¸°ê°„ì˜ ì „ì²´ ì¶”ì´ë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        3. ì¼ë³„ íŠ¸ë Œë“œ (daily_trend.png, daily_analysis.csv):
           - ì¼ìë³„ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ë³€í™”ë¥¼ ì„  ê·¸ë˜í”„ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        4. ì£¼ë³„ íŠ¸ë Œë“œ (weekly_trend.png, weekly_analysis.csv):
           - ë§¤ì£¼ (ì¼ìš”ì¼ ê¸°ì¤€) ê°„ê²©ìœ¼ë¡œ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ì¶”ì´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

        5. ìš”ì¼ë³„ ë¶„ì„ (day_of_week_views.png, day_of_week_analysis.csv):
           - ì›”/ì¼ ë‹¨ìœ„ê°€ ì•„ë‹Œ ì¼ì£¼ì¼ ê°„ì˜ íŠ¹ì • ìš”ì¼(Mon~Sun)ë³„ë¡œ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
           - ì—…ë¡œë“œí•˜ê¸° ì¢‹ì€ ìš”ì¼ ë“±ì„ íŒŒì•…í•  ë•Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        6. ìƒìœ„ 10ê°œ ì±„ë„ (top_channels_views.png, channel_analysis.csv):
           - 'YouTube Channel'ë³„ë¡œ ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ì´í•©ì„ êµ¬í•œ ë’¤, ì¡°íšŒìˆ˜ê°€ ë†’ì€ ìƒìœ„ 10ê°œ ì±„ë„ì„ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

        7. ìƒìœ„ 10ê°œ ì˜ìƒ (top_10_videos.png, top_10_videos.csv):
           - ì¡°íšŒìˆ˜ê°€ ê°€ì¥ ë†’ì€ 10ê°œ ì˜ìƒì˜ ì œëª©, ì±„ë„ëª…, ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€ ìˆ˜ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

        8. Like-View ë¹„ìœ¨ & Comment-View ë¹„ìœ¨ ë¶„í¬ (like_view_ratio_distribution.png, comment_view_ratio_distribution.png):
           - ê·¹ë‹¨ê°’(ìƒìœ„ 1% êµ¬ê°„)ì„ ì˜ë¼ë‚¸ ë’¤, (likes / views), (comments_count / views)ì˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
           - ë¡œê·¸ ìŠ¤ì¼€ì¼ ë³€í™˜ ë“±ìœ¼ë¡œ ì¶”ê°€ ë¶„ì„ ê°€ëŠ¥.

        9. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ (correlation_matrix.png, correlation_matrix.csv):
           - views, likes, comments_count, like_view_ratio, comment_view_ratio ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì˜ˆ: Like-View ë¹„ìœ¨ê³¼ Comment-View ë¹„ìœ¨ì´ ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ”ì§€, Likesì™€ Views ê°„ì— ì–´ë–¤ ìƒê´€ì´ ìˆëŠ”ì§€ ë“±ì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        description_file_path = os.path.join(output_dir, "description.txt")
        with open(safe_path(description_file_path), 'w', encoding="utf-8", errors="ignore") as file:
            file.write(description_text)
        return True

    def YouTubeReplyAnalysis(self, data, file_path):
        if not self.checkColumns([
            'Reply Num', 
            'Reply Writer', 
            'Reply Date',
            'Reply Text', 
            'Reply Like', 
            'Article URL', 
            'Article Day'
        ], data.columns):
            return False

        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 1) ë‚ ì§œí˜• / ìˆ«ìí˜• ë³€í™˜
        # - ëŒ“ê¸€ì´ ì‘ì„±ëœ ë‚ ì§œ
        data["Reply Date"] = pd.to_datetime(
            data["Reply Date"], errors="coerce")
        # - ê²Œì‹œë¬¼ì´ ì˜¬ë¼ì˜¨ ë‚ ì§œ
        data["Article Day"] = pd.to_datetime(
            data["Article Day"], errors="coerce")

        # - ì¢‹ì•„ìš” ìˆ˜: ìˆ«ì ë³€í™˜
        data["Reply Like"] = pd.to_numeric(
            data["Reply Like"], errors="coerce").fillna(0)

        # 2) ê²°ê³¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(
            os.path.dirname(file_path),
            os.path.basename(file_path).replace(".csv", "") + "_analysis"
        )
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # 3) ê¸°ë³¸ í†µê³„ (ê¸°ìˆ  í†µê³„ëŸ‰)
        basic_stats = data.describe(include="all")  # ë²”ì£¼í˜•/ìˆ˜ì¹˜í˜• ëª¨ë‘ ê¸°ìˆ í†µê³„

        # 4) ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ë§Œ ë”°ë¡œ ê´€ë¦¬
        #    (ëŒ“ê¸€ ë‚ ì§œ, ê¸°ì‚¬ ë‚ ì§œ ëª¨ë‘ ì œëŒ€ë¡œ ë³€í™˜ëœ í–‰ë§Œ ë¶„ì„ì— í™œìš©)
        valid_data = data.dropna(subset=["Reply Date", "Article Day"]).copy()

        # 5) ë‚ ì§œ ì°¨ì´(ëŒ“ê¸€ ì‘ì„± ì‹œì  vs ê²Œì‹œë¬¼ ì—…ë¡œë“œ ì‹œì )
        #    -> 'ì‘ì„± ì‹œì  - ì—…ë¡œë“œ ì‹œì ' ì¼ìˆ˜ ê³„ì‚°
        valid_data["ReplyTimeDelta"] = (
            valid_data["Reply Date"] - valid_data["Article Day"]).dt.days

        #    ì˜ˆ: ReplyTimeDelta = 0 ì´ë©´ ê°™ì€ ë‚  ì˜¬ë¼ì˜¨ ëŒ“ê¸€
        #        ReplyTimeDelta = 1 ì´ë©´ ì—…ë¡œë“œ ë‹¤ìŒ ë‚  ë‹¬ë¦° ëŒ“ê¸€
        #        ìŒìˆ˜ê°€ ë‚˜ì˜¤ë©´ ì—…ë¡œë“œ ì „ ì‹œì (ì˜ëª»ëœ ë°ì´í„°)ì¼ ìˆ˜ë„ ìˆìŒ

        # 6) ê·¸ë£¹í™” ë¶„ì„
        #    6-1) ì¼ë³„ ëŒ“ê¸€ ì¶”ì´
        daily_data = valid_data.groupby(valid_data["Reply Date"].dt.to_period("D")).agg(
            reply_count=("Reply Text", "count"),
            total_like=("Reply Like", "sum"),
            avg_time_diff=("ReplyTimeDelta", "mean")  # ì¼ë³„ë¡œ ëŒ“ê¸€-ê²Œì‹œë¬¼ ê°„ í‰ê·  ì‹œì°¨
        ).reset_index()
        daily_data["Reply Date"] = daily_data["Reply Date"].dt.to_timestamp()

        #    6-2) ì›”ë³„ ëŒ“ê¸€ ì¶”ì´
        monthly_data = valid_data.groupby(valid_data["Reply Date"].dt.to_period("M")).agg(
            reply_count=("Reply Text", "count"),
            total_like=("Reply Like", "sum"),
            avg_time_diff=("ReplyTimeDelta", "mean")
        ).reset_index()
        monthly_data["Reply Date"] = monthly_data["Reply Date"].dt.to_timestamp()

        #    6-3) ìš”ì¼ë³„ ë¶„ì„ (ëŒ“ê¸€ ì‘ì„± ìš”ì¼)
        valid_data["ReplyDayOfWeek"] = valid_data["Reply Date"].dt.day_name()
        dow_data = valid_data.groupby("ReplyDayOfWeek").agg(
            reply_count=("Reply Text", "count"),
            total_like=("Reply Like", "sum"),
            avg_time_diff=("ReplyTimeDelta", "mean")
        ).reset_index()

        #    6-4) ê²Œì‹œë¬¼(Article URL)ë³„ ë¶„ì„
        article_analysis = data.groupby("Article URL").agg(
            reply_count=("Reply Text", "count"),
            total_like=("Reply Like", "sum")
        ).reset_index()

        #    6-5) ê²Œì‹œë¬¼ ì—…ë¡œë“œ ë‚ ì§œ(Article Day) ê¸°ì¤€ ë¶„ì„
        #         ì—…ë¡œë“œ ë‚ ì§œê°€ ê°™ìœ¼ë©´ ê°™ì€ ë‚  ì—…ë¡œë“œëœ ë‹¤ë¥¸ ê²Œì‹œë¬¼ë¡œ ê°„ì£¼
        #         ë‚ ì§œ ë³€í™˜ ì•ˆ ëœê±´ ì œì™¸(valid_dataë§Œ ì‚¬ìš© ê°€ëŠ¥)
        day_post_analysis = valid_data.groupby(valid_data["Article Day"].dt.to_period("D")).agg(
            article_reply_count=("Reply Text", "count"),
            article_reply_like=("Reply Like", "sum"),
            avg_reply_time=("ReplyTimeDelta", "mean")  # ì—…ë¡œë“œì¼ ê¸°ì¤€ í‰ê·  ëŒ“ê¸€ ì‹œì°¨
        ).reset_index()
        day_post_analysis["Article Day"] = day_post_analysis["Article Day"].dt.to_timestamp(
        )

        #    6-6) ëŒ“ê¸€ ì‘ì„±ìë³„(Reply Writer) ë¶„ì„
        writer_analysis = data.groupby("Reply Writer").agg(
            reply_count=("Reply Text", "count"),
            total_like=("Reply Like", "sum")
        ).reset_index()

        # 7) ìƒìœ„ 10ê°œ í•­ëª©
        #    - ì‘ì„±ì, ê²Œì‹œë¬¼
        top_10_writers = writer_analysis.sort_values(
            "reply_count", ascending=False).head(10)
        top_10_articles = article_analysis.sort_values(
            "reply_count", ascending=False).head(10)

        # 8) ìƒìœ„ 10ê°œ ëŒ“ê¸€(ì¢‹ì•„ìš” ê¸°ì¤€)
        top_10_liked_replies = data.sort_values("Reply Like", ascending=False).head(10)[
            ["Reply Writer", "Reply Text", "Reply Date",
                "Reply Like", "Article URL", "Article Day"]
        ].reset_index(drop=True)

        # 9) í†µê³„ ì§€í‘œ í™•ì¥
        #    - ì˜ˆ: Reply Like ë¶„í¬ ì‹œê°í™”ë¥¼ ìœ„í•´ ìƒìœ„ 1% ìë¥´ê¸°
        #    - ì½”ë¦´ë ˆì´ì…˜ì€ Likeì™€ TimeDelta ì •ë„ë§Œ í•´ë³¼ ìˆ˜ ìˆìŒ
        numeric_cols = ["Reply Like"]
        # ReplyTimeDeltaë„ ìˆ«ìí˜•ì´ë©´ ìƒê´€ê´€ê³„ì— ì¶”ê°€
        if "ReplyTimeDelta" in valid_data.columns:
            numeric_cols.append("ReplyTimeDelta")

        # ìƒê´€ê´€ê³„ (valid_dataë§Œ ì‚¬ìš©í•´ë„ ë¨, ì—¬ê¸°ì„œëŠ” ì „ì²´ data ì¤‘ null ì œì™¸)
        # nullì´ ìˆìœ¼ë©´ corr() ê³„ì‚°ì—ì„œ ì œì™¸ë¨.
        correlation_matrix = valid_data[numeric_cols].corr()

        # 10) CSV ì €ì¥
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding="utf-8-sig")
        daily_data.to_csv(os.path.join(
            csv_output_dir, "daily_analysis.csv"), encoding="utf-8-sig", index=False)
        monthly_data.to_csv(os.path.join(
            csv_output_dir, "monthly_analysis.csv"), encoding="utf-8-sig", index=False)
        dow_data.to_csv(os.path.join(
            csv_output_dir, "day_of_week_analysis.csv"), encoding="utf-8-sig", index=False)
        article_analysis.to_csv(os.path.join(
            csv_output_dir, "article_analysis.csv"), encoding="utf-8-sig", index=False)
        day_post_analysis.to_csv(os.path.join(csv_output_dir, "article_day_analysis.csv"), encoding="utf-8-sig",
                                 index=False)
        writer_analysis.to_csv(os.path.join(
            csv_output_dir, "writer_analysis.csv"), encoding="utf-8-sig", index=False)
        top_10_writers.to_csv(os.path.join(
            csv_output_dir, "top_10_writers.csv"), encoding="utf-8-sig", index=False)
        top_10_articles.to_csv(os.path.join(
            csv_output_dir, "top_10_articles.csv"), encoding="utf-8-sig", index=False)
        top_10_liked_replies.to_csv(os.path.join(csv_output_dir, "top_10_liked_replies.csv"), encoding="utf-8-sig",
                                    index=False)
        correlation_matrix.to_csv(os.path.join(
            csv_output_dir, "correlation_matrix.csv"), encoding="utf-8-sig")

        # 11) ì‹œê°í™”
        #     - calculate_figsize(len(x)) í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •. (ì—†ìœ¼ë©´ (10,6) ë“± ì§ì ‘ ì…ë ¥)
        # (1) ì¼ë³„ ëŒ“ê¸€ ì¶”ì´
        plt.figure(figsize=self.calculate_figsize(len(daily_data)))
        sns.lineplot(data=daily_data, x="Reply Date",
                     y="reply_count", label="Reply Count")
        sns.lineplot(data=daily_data, x="Reply Date",
                     y="total_like", label="Total Like")
        plt.title("ì¼ë³„ ëŒ“ê¸€/ì¢‹ì•„ìš” ì¶”ì´")
        plt.xlabel("ë‚ ì§œ")
        plt.ylabel("í•©ê³„")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "daily_trend.png"))
        plt.close()

        # (2) ì›”ë³„ ëŒ“ê¸€ ì¶”ì´
        plt.figure(figsize=self.calculate_figsize(len(monthly_data)))
        sns.lineplot(data=monthly_data, x="Reply Date",
                     y="reply_count", label="Reply Count")
        sns.lineplot(data=monthly_data, x="Reply Date",
                     y="total_like", label="Total Like")
        plt.title("ì›”ë³„ ëŒ“ê¸€/ì¢‹ì•„ìš” ì¶”ì´")
        plt.xlabel("ì›”")
        plt.ylabel("í•©ê³„")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "monthly_trend.png"))
        plt.close()

        # (3) ìš”ì¼ë³„ ëŒ“ê¸€ ìˆ˜
        dow_order = ["Monday", "Tuesday", "Wednesday",
                     "Thursday", "Friday", "Saturday", "Sunday"]
        dow_data["ReplyDayOfWeek"] = pd.Categorical(
            dow_data["ReplyDayOfWeek"], categories=dow_order, ordered=True)
        sorted_dow_data = dow_data.sort_values("ReplyDayOfWeek")

        plt.figure(figsize=self.calculate_figsize(len(sorted_dow_data)))
        sns.barplot(data=sorted_dow_data, x="ReplyDayOfWeek", y="reply_count")
        plt.title("ìš”ì¼ë³„ ëŒ“ê¸€ ìˆ˜")
        plt.xlabel("ìš”ì¼")
        plt.ylabel("ëŒ“ê¸€ ìˆ˜")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "day_of_week_reply_count.png"))
        plt.close()

        # (4) Article Day ê¸°ì¤€ (ê²Œì‹œë¬¼ ì—…ë¡œë“œ ë‚ ì§œë³„) ëŒ“ê¸€ ìˆ˜
        plt.figure(figsize=self.calculate_figsize(len(day_post_analysis)))
        sns.lineplot(data=day_post_analysis, x="Article Day",
                     y="article_reply_count", label="Reply Count")
        sns.lineplot(data=day_post_analysis, x="Article Day",
                     y="article_reply_like", label="Reply Like")
        plt.title("ì˜ìƒ ì—…ë¡œë“œ ë‚ ì§œë³„ ëŒ“ê¸€ ìˆ˜/ì¢‹ì•„ìš” ì¶”ì´")
        plt.xlabel("ì˜ìƒ ì—…ë¡œë“œ ë‚ ì§œ")
        plt.ylabel("í•©ê³„")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "article_day_trend.png"))
        plt.close()

        # (5) ëŒ“ê¸€ ì‘ì„±ì (ìƒìœ„ 10ëª…)
        plt.figure(figsize=self.calculate_figsize(len(top_10_writers)))
        sns.barplot(data=top_10_writers, x="Reply Writer", y="reply_count")
        plt.title("ìƒìœ„ 10ëª… ëŒ“ê¸€ ì‘ì„±ì")
        plt.xlabel("ì‘ì„±ì")
        plt.ylabel("ëŒ“ê¸€ ìˆ˜")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_10_writers.png"))
        plt.close()

        # (6) ê²Œì‹œë¬¼(Article URL)ë³„ ëŒ“ê¸€ ìˆ˜ (ìƒìœ„ 10)
        plt.figure(figsize=self.calculate_figsize(len(top_10_articles)))
        sns.barplot(data=top_10_articles, x="Article URL", y="reply_count")
        plt.title("ìƒìœ„ 10 ì˜ìƒë³„ ëŒ“ê¸€ ìˆ˜")
        plt.xlabel("Article URL")
        plt.ylabel("ëŒ“ê¸€ ìˆ˜")
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_10_articles.png"))
        plt.close()

        # (8) ReplyTimeDelta(ëŒ“ê¸€ ì‘ì„± - ì—…ë¡œë“œ ë‚ ì§œ)ì˜ ë¶„í¬
        #     0ë³´ë‹¤ ì‘ìœ¼ë©´ ì—…ë¡œë“œ ì´ì „ì— ì‘ì„±ëœ(?) ì´ìƒì¹˜ì¼ ìˆ˜ ìˆìŒ
        plt.figure(figsize=self.calculate_figsize(10))
        ax2 = sns.histplot(data=valid_data, x="ReplyTimeDelta", kde=True)
        # ìƒìœ„ 1% ì˜ë¼ë‚´ê³  ì‹¶ë‹¤ë©´:
        delta_99 = valid_data["ReplyTimeDelta"].quantile(0.99)
        delta_min = valid_data["ReplyTimeDelta"].min()  # ìŒìˆ˜ë„ ìˆì„ ìˆ˜ ìˆìŒ
        ax2.set_xlim(delta_min, delta_99)
        plt.title("ëŒ“ê¸€-ì˜ìƒ ì‹œì°¨(ì¼) ë¶„í¬ (ìƒìœ„ 1% ì œì™¸)")
        plt.xlabel("ReplyTimeDelta (Days)")
        plt.ylabel("ë¹ˆë„")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "reply_time_delta_distribution.png"))
        plt.close()

        # (9) ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.figure(figsize=self.calculate_figsize(
            len(correlation_matrix), height=8))
        sns.heatmap(correlation_matrix, annot=True,
                    cmap="coolwarm", vmin=-1, vmax=1)
        plt.title("ëŒ“ê¸€ ë°ì´í„° ìƒê´€ê´€ê³„")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "correlation_matrix.png"))
        plt.close()

        # 12) ë¶„ì„/ê·¸ë˜í”„ ì„¤ëª… txt
        description_text = """
        [ëŒ“ê¸€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì„¤ëª…]

        1. basic_stats.csv
           - ì „ì²´ CSV ë°ì´í„°ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„(ìµœì†Ÿê°’, ìµœëŒ“ê°’, í‰ê·  ë“±).

        2. daily_analysis.csv / daily_trend.png
           - ì¼ì(Reply Date) ê¸°ì¤€ìœ¼ë¡œ ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ í•©ê³„ë¥¼ ì„  ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

        3. monthly_analysis.csv / monthly_trend.png
           - ì›”ë³„ ëŒ“ê¸€/ì¢‹ì•„ìš” ì¶”ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        4. day_of_week_analysis.csv / day_of_week_reply_count.png
           - ìš”ì¼ë³„(Monday~Sunday) ëŒ“ê¸€ ìˆ˜ë¥¼ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.

        5. article_day_analysis.csv / article_day_trend.png
           - 'Article Day'(ì˜ìƒì´ ì˜¬ë¼ì˜¨ ë‚ ì§œ)ë³„ ëŒ“ê¸€ ìˆ˜/ì¢‹ì•„ìš” ì¶”ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì—…ë¡œë“œ í›„ ëŒ“ê¸€ì´ ì–¸ì œ ë§ì´ ë‹¬ë¦¬ëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

        6. article_analysis.csvì´ì •ë¦¬í•œ CSV.

        7. writer_analysis.csv
           - 'Reply Writer'(ì‘ì„±ì)ë³„ë¡œ ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ í•©ê³„ë¥¼ ë¶„ì„í•œ CSV.

        8. top_10_writers.csv / top_10_writers.png
           - ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ëª… ì‘ì„±ì ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³ , ë°” ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

        9. top_10_articles.csv / top_10_articles.png
           - ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ê°œ Article URLì„ ì •ë¦¬í•˜ê³ , ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

        10. top_10_liked_replies.csv
            - ì¢‹ì•„ìš”(Reply Like)ê°€ ê°€ì¥ ë§ì€ ëŒ“ê¸€ 10ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        11. reply_like_distribution.png
            - ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜ì˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ê³¼ KDEê³¡ì„ ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
            - ìƒìœ„ 1% êµ¬ê°„ì€ ì˜ë¼ë‚´ì–´ xì¶• ë²”ìœ„ë¥¼ ì œí•œí–ˆìŠµë‹ˆë‹¤.

        12. reply_time_delta_distribution.png
            - (ëŒ“ê¸€ ì‘ì„± ë‚ ì§œ - ê²Œì‹œë¬¼ ì—…ë¡œë“œ ë‚ ì§œ)ë¥¼ ì¼(day) ë‹¨ìœ„ë¡œ ê³„ì‚°í•œ ì‹œì°¨ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.

        13. correlation_matrix.csv / correlation_matrix.png
            - 'Reply Like', 'ReplyTimeDelta' ë“±ì˜ ìˆ˜ì¹˜ ì»¬ëŸ¼ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

        """
        with open(safe_path(os.path.join(output_dir, "description.txt")), "w", encoding="utf-8", errors="ignore") as f:
            f.write(description_text)
        return True

    def YouTubeRereplyAnalysis(self, data, file_path):
        if not self.checkColumns([
            'Rereply Num', 
            'Rereply Writer', 
            'Rereply Date',
            'Rereply Text', 
            'Rereply Like', 
            'Article URL', 
            'Article Day'
        ], data.columns):
            return False
        
        if 'id' not in data.columns:
            # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ë²ˆí˜¸ë¥¼ ë¶€ì—¬
            data.insert(0, 'id', range(1, len(data) + 1))

        # 1) ë‚ ì§œí˜• / ìˆ«ìí˜• ë³€í™˜
        # - ëŒ€ëŒ“ê¸€ì´ ì‘ì„±ëœ ë‚ ì§œ
        data["Rereply Date"] = pd.to_datetime(
            data["Rereply Date"], errors="coerce")
        # - ê²Œì‹œë¬¼ì´ ì˜¬ë¼ì˜¨ ë‚ ì§œ
        data["Article Day"] = pd.to_datetime(
            data["Article Day"], errors="coerce")

        # - ì¢‹ì•„ìš” ìˆ˜: ìˆ«ì ë³€í™˜
        data["Rereply Like"] = pd.to_numeric(
            data["Rereply Like"], errors="coerce").fillna(0)

        # 2) ê²°ê³¼ ì €ì¥ìš© ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(
            os.path.dirname(file_path),
            os.path.basename(file_path).replace(".csv", "") + "_analysis"
        )
        csv_output_dir = os.path.join(output_dir, "csv_files")
        graph_output_dir = os.path.join(output_dir, "graphs")
        os.makedirs(csv_output_dir, exist_ok=True)
        os.makedirs(graph_output_dir, exist_ok=True)

        # 3) ê¸°ë³¸ í†µê³„ (ê¸°ìˆ  í†µê³„ëŸ‰)
        basic_stats = data.describe(include="all")  # ë²”ì£¼í˜•/ìˆ˜ì¹˜í˜• ëª¨ë‘ ê¸°ìˆ í†µê³„

        # 4) ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ë§Œ ë”°ë¡œ ê´€ë¦¬
        #    (ëŒ€ëŒ“ê¸€ ë‚ ì§œ, ì˜ìƒ ë‚ ì§œ ëª¨ë‘ ì œëŒ€ë¡œ ë³€í™˜ëœ í–‰ë§Œ ë¶„ì„ì— í™œìš©)
        valid_data = data.dropna(subset=["Rereply Date", "Article Day"]).copy()

        # 5) ë‚ ì§œ ì°¨ì´(ëŒ€ëŒ“ê¸€ ì‘ì„± ì‹œì  vs ê²Œì‹œë¬¼ ì—…ë¡œë“œ ì‹œì )
        #    -> 'ì‘ì„± ì‹œì  - ì—…ë¡œë“œ ì‹œì ' ì¼ìˆ˜ ê³„ì‚°
        valid_data["RereplyTimeDelta"] = (
            valid_data["Rereply Date"] - valid_data["Article Day"]).dt.days
        #    ì˜ˆ: RereplyTimeDelta = 0 ì´ë©´ ê°™ì€ ë‚  ì˜¬ë¼ì˜¨ ëŒ€ëŒ“ê¸€
        #        RereplyTimeDelta = 1 ì´ë©´ ì—…ë¡œë“œ ë‹¤ìŒ ë‚  ë‹¬ë¦° ëŒ€ëŒ“ê¸€
        #        ìŒìˆ˜ê°€ ë‚˜ì˜¤ë©´ ì—…ë¡œë“œ ì „ ì‹œì (ì˜ëª»ëœ ë°ì´í„°)ì¼ ìˆ˜ë„ ìˆìŒ

        # 6) ê·¸ë£¹í™” ë¶„ì„
        #    6-1) ì¼ë³„ ëŒ€ëŒ“ê¸€ ì¶”ì´
        daily_data = valid_data.groupby(valid_data["Rereply Date"].dt.to_period("D")).agg(
            rereply_count=("Rereply Text", "count"),
            total_like=("Rereply Like", "sum"),
            avg_time_diff=("RereplyTimeDelta", "mean")  # ì¼ë³„ë¡œ ëŒ€ëŒ“ê¸€-ê²Œì‹œë¬¼ ê°„ í‰ê·  ì‹œì°¨
        ).reset_index()
        daily_data["Rereply Date"] = daily_data["Rereply Date"].dt.to_timestamp()

        #    6-2) ì›”ë³„ ëŒ€ëŒ“ê¸€ ì¶”ì´
        monthly_data = valid_data.groupby(valid_data["Rereply Date"].dt.to_period("M")).agg(
            rereply_count=("Rereply Text", "count"),
            total_like=("Rereply Like", "sum"),
            avg_time_diff=("RereplyTimeDelta", "mean")
        ).reset_index()
        monthly_data["Rereply Date"] = monthly_data["Rereply Date"].dt.to_timestamp()

        #    6-3) ìš”ì¼ë³„ ë¶„ì„ (ëŒ€ëŒ“ê¸€ ì‘ì„± ìš”ì¼)
        valid_data["RereplyDayOfWeek"] = valid_data["Rereply Date"].dt.day_name()
        dow_data = valid_data.groupby("RereplyDayOfWeek").agg(
            rereply_count=("Rereply Text", "count"),
            total_like=("Rereply Like", "sum"),
            avg_time_diff=("RereplyTimeDelta", "mean")
        ).reset_index()

        #    6-4) ê²Œì‹œë¬¼(Article URL)ë³„ ë¶„ì„
        article_analysis = data.groupby("Article URL").agg(
            rereply_count=("Rereply Text", "count"),
            total_like=("Rereply Like", "sum")
        ).reset_index()

        #    6-5) ê²Œì‹œë¬¼ ì—…ë¡œë“œ ë‚ ì§œ(Article Day) ê¸°ì¤€ ë¶„ì„
        day_post_analysis = valid_data.groupby(valid_data["Article Day"].dt.to_period("D")).agg(
            article_rereply_count=("Rereply Text", "count"),
            article_rereply_like=("Rereply Like", "sum"),
            avg_rereply_time=("RereplyTimeDelta", "mean")  # ì—…ë¡œë“œì¼ ê¸°ì¤€ í‰ê·  ëŒ€ëŒ“ê¸€ ì‹œì°¨
        ).reset_index()
        day_post_analysis["Article Day"] = day_post_analysis["Article Day"].dt.to_timestamp(
        )

        #    6-6) ëŒ€ëŒ“ê¸€ ì‘ì„±ìë³„(Rereply Writer) ë¶„ì„
        writer_analysis = data.groupby("Rereply Writer").agg(
            rereply_count=("Rereply Text", "count"),
            total_like=("Rereply Like", "sum")
        ).reset_index()

        # 7) ìƒìœ„ 10ê°œ í•­ëª©
        #    - ì‘ì„±ì, ê²Œì‹œë¬¼
        top_10_writers = writer_analysis.sort_values(
            "rereply_count", ascending=False).head(10)
        top_10_articles = article_analysis.sort_values(
            "rereply_count", ascending=False).head(10)

        # 8) ìƒìœ„ 10ê°œ ëŒ€ëŒ“ê¸€(ì¢‹ì•„ìš” ê¸°ì¤€)
        top_10_liked_rereplies = data.sort_values("Rereply Like", ascending=False).head(10)[
            ["Rereply Writer", "Rereply Text", "Rereply Date",
                "Rereply Like", "Article URL", "Article Day"]
        ].reset_index(drop=True)

        # 9) í†µê³„ ì§€í‘œ í™•ì¥
        #    - ì˜ˆ: Rereply Like ë¶„í¬ ì‹œê°í™”ë¥¼ ìœ„í•´ ìƒìœ„ 1% ìë¥´ê¸°
        #    - ì½”ë¦´ë ˆì´ì…˜ì€ Likeì™€ TimeDelta ì •ë„ë§Œ í•´ë³¼ ìˆ˜ ìˆìŒ
        numeric_cols = ["Rereply Like"]
        # RereplyTimeDeltaë„ ìˆ«ìí˜•ì´ë©´ ìƒê´€ê´€ê³„ì— ì¶”ê°€
        if "RereplyTimeDelta" in valid_data.columns:
            numeric_cols.append("RereplyTimeDelta")

        # ìƒê´€ê´€ê³„ (valid_dataë§Œ ì‚¬ìš©)
        correlation_matrix = valid_data[numeric_cols].corr()

        # 10) CSV ì €ì¥
        basic_stats.to_csv(os.path.join(
            csv_output_dir, "basic_stats.csv"), encoding="utf-8-sig")
        daily_data.to_csv(os.path.join(
            csv_output_dir, "daily_analysis.csv"), encoding="utf-8-sig", index=False)
        monthly_data.to_csv(os.path.join(
            csv_output_dir, "monthly_analysis.csv"), encoding="utf-8-sig", index=False)
        dow_data.to_csv(os.path.join(
            csv_output_dir, "day_of_week_analysis.csv"), encoding="utf-8-sig", index=False)
        article_analysis.to_csv(os.path.join(
            csv_output_dir, "article_analysis.csv"), encoding="utf-8-sig", index=False)
        day_post_analysis.to_csv(os.path.join(csv_output_dir, "article_day_analysis.csv"), encoding="utf-8-sig",
                                 index=False)
        writer_analysis.to_csv(os.path.join(
            csv_output_dir, "writer_analysis.csv"), encoding="utf-8-sig", index=False)
        top_10_writers.to_csv(os.path.join(
            csv_output_dir, "top_10_writers.csv"), encoding="utf-8-sig", index=False)
        top_10_articles.to_csv(os.path.join(
            csv_output_dir, "top_10_articles.csv"), encoding="utf-8-sig", index=False)
        top_10_liked_rereplies.to_csv(os.path.join(csv_output_dir, "top_10_liked_rereplies.csv"), encoding="utf-8-sig",
                                      index=False)
        correlation_matrix.to_csv(os.path.join(
            csv_output_dir, "correlation_matrix.csv"), encoding="utf-8-sig")

        # 11) ì‹œê°í™”
        #     - calculate_figsize(len(x)) í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •. (ì—†ìœ¼ë©´ (10,6) ë“± ì§ì ‘ ì…ë ¥)
        # (1) ì¼ë³„ ëŒ€ëŒ“ê¸€ ì¶”ì´
        plt.figure(figsize=self.calculate_figsize(len(daily_data)))
        sns.lineplot(data=daily_data, x="Rereply Date",
                     y="rereply_count", label="Rereply Count")
        sns.lineplot(data=daily_data, x="Rereply Date",
                     y="total_like", label="Total Like")
        plt.title("ì¼ë³„ ëŒ€ëŒ“ê¸€/ì¢‹ì•„ìš” ì¶”ì´")
        plt.xlabel("ë‚ ì§œ")
        plt.ylabel("í•©ê³„")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "daily_trend.png"))
        plt.close()

        # (2) ì›”ë³„ ëŒ€ëŒ“ê¸€ ì¶”ì´
        plt.figure(figsize=self.calculate_figsize(len(monthly_data)))
        sns.lineplot(data=monthly_data, x="Rereply Date",
                     y="rereply_count", label="Rereply Count")
        sns.lineplot(data=monthly_data, x="Rereply Date",
                     y="total_like", label="Total Like")
        plt.title("ì›”ë³„ ëŒ€ëŒ“ê¸€/ì¢‹ì•„ìš” ì¶”ì´")
        plt.xlabel("ì›”")
        plt.ylabel("í•©ê³„")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "monthly_trend.png"))
        plt.close()

        # (3) ìš”ì¼ë³„ ëŒ€ëŒ“ê¸€ ìˆ˜
        dow_order = ["Monday", "Tuesday", "Wednesday",
                     "Thursday", "Friday", "Saturday", "Sunday"]
        dow_data["RereplyDayOfWeek"] = pd.Categorical(
            dow_data["RereplyDayOfWeek"], categories=dow_order, ordered=True)
        sorted_dow_data = dow_data.sort_values("RereplyDayOfWeek")

        plt.figure(figsize=self.calculate_figsize(len(sorted_dow_data)))
        sns.barplot(data=sorted_dow_data,
                    x="RereplyDayOfWeek", y="rereply_count")
        plt.title("ìš”ì¼ë³„ ëŒ€ëŒ“ê¸€ ìˆ˜")
        plt.xlabel("ìš”ì¼")
        plt.ylabel("ëŒ€ëŒ“ê¸€ ìˆ˜")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "day_of_week_rereply_count.png"))
        plt.close()

        # (4) Article Day ê¸°ì¤€ (ê²Œì‹œë¬¼ ì—…ë¡œë“œ ë‚ ì§œë³„) ëŒ€ëŒ“ê¸€ ìˆ˜
        plt.figure(figsize=self.calculate_figsize(len(day_post_analysis)))
        sns.lineplot(data=day_post_analysis, x="Article Day",
                     y="article_rereply_count", label="Rereply Count")
        sns.lineplot(data=day_post_analysis, x="Article Day",
                     y="article_rereply_like", label="Rereply Like")
        plt.title("ì˜ìƒ ì—…ë¡œë“œ ë‚ ì§œë³„ ëŒ€ëŒ“ê¸€ ìˆ˜/ì¢‹ì•„ìš” ì¶”ì´")
        plt.xlabel("ì˜ìƒ ì—…ë¡œë“œ ë‚ ì§œ")
        plt.ylabel("í•©ê³„")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "article_day_trend.png"))
        plt.close()

        # (5) ëŒ€ëŒ“ê¸€ ì‘ì„±ì (ìƒìœ„ 10ëª…)
        plt.figure(figsize=self.calculate_figsize(len(top_10_writers)))
        sns.barplot(data=top_10_writers, x="Rereply Writer", y="rereply_count")
        plt.title("ìƒìœ„ 10ëª… ëŒ€ëŒ“ê¸€ ì‘ì„±ì")
        plt.xlabel("ì‘ì„±ì")
        plt.ylabel("ëŒ€ëŒ“ê¸€ ìˆ˜")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_10_writers.png"))
        plt.close()

        # (6) ê²Œì‹œë¬¼(Article URL)ë³„ ëŒ€ëŒ“ê¸€ ìˆ˜ (ìƒìœ„ 10)
        plt.figure(figsize=self.calculate_figsize(len(top_10_articles)))
        sns.barplot(data=top_10_articles, x="Article URL", y="rereply_count")
        plt.title("ìƒìœ„ 10 ì˜ìƒë³„ ëŒ€ëŒ“ê¸€ ìˆ˜")
        plt.xlabel("Article URL")
        plt.ylabel("ëŒ€ëŒ“ê¸€ ìˆ˜")
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "top_10_articles.png"))
        plt.close()

        # (8) RereplyTimeDelta(ëŒ€ëŒ“ê¸€ ì‘ì„± - ì—…ë¡œë“œ ë‚ ì§œ)ì˜ ë¶„í¬
        plt.figure(figsize=self.calculate_figsize(10))
        ax2 = sns.histplot(data=valid_data, x="RereplyTimeDelta", kde=True)
        # ìƒìœ„ 1% ì˜ë¼ë‚´ê³  ì‹¶ë‹¤ë©´:
        delta_99 = valid_data["RereplyTimeDelta"].quantile(0.99)
        delta_min = valid_data["RereplyTimeDelta"].min()
        ax2.set_xlim(delta_min, delta_99)
        plt.title("ëŒ€ëŒ“ê¸€-ì˜ìƒ ì‹œì°¨(ì¼) ë¶„í¬ (ìƒìœ„ 1% ì œì™¸)")
        plt.xlabel("RereplyTimeDelta (Days)")
        plt.ylabel("ë¹ˆë„")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir,
                    "rereply_time_delta_distribution.png"))
        plt.close()

        # (9) ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
        plt.figure(figsize=self.calculate_figsize(
            len(correlation_matrix), height=8))
        sns.heatmap(correlation_matrix, annot=True,
                    cmap="coolwarm", vmin=-1, vmax=1)
        plt.title("ëŒ€ëŒ“ê¸€ ë°ì´í„° ìƒê´€ê´€ê³„")
        plt.tight_layout()
        plt.savefig(os.path.join(graph_output_dir, "correlation_matrix.png"))
        plt.close()

        # 12) ë¶„ì„/ê·¸ë˜í”„ ì„¤ëª… txt
        description_text = """
        [ëŒ€ëŒ“ê¸€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì„¤ëª…]

        1. basic_stats.csv
           - ì „ì²´ CSV ë°ì´í„°ì— ëŒ€í•œ ê¸°ë³¸ í†µê³„(ìµœì†Ÿê°’, ìµœëŒ“ê°’, í‰ê·  ë“±).

        2. daily_analysis.csv / daily_trend.png
           - ì¼ì(Rereply Date) ê¸°ì¤€ìœ¼ë¡œ ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ í•©ê³„ë¥¼ ì„  ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

        3. monthly_analysis.csv / monthly_trend.png
           - ì›”ë³„ ëŒ€ëŒ“ê¸€/ì¢‹ì•„ìš” ì¶”ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        4. day_of_week_analysis.csv / day_of_week_rereply_count.png
           - ìš”ì¼ë³„(Monday~Sunday) ëŒ€ëŒ“ê¸€ ìˆ˜ë¥¼ ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í–ˆìŠµë‹ˆë‹¤.

        5. article_day_analysis.csv / article_day_trend.png
           - 'Article Day'(ì˜ìƒì´ ì˜¬ë¼ì˜¨ ë‚ ì§œ)ë³„ ëŒ€ëŒ“ê¸€ ìˆ˜/ì¢‹ì•„ìš” ì¶”ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
           - ì—…ë¡œë“œ í›„ ëŒ€ëŒ“ê¸€ì´ ì–¸ì œ ë§ì´ ë‹¬ë¦¬ëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

        6. article_analysis.csv
           - Article URLë³„ ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ í•©ê³„ë¥¼ ì •ë¦¬í•œ CSV.

        7. writer_analysis.csv
           - 'Rereply Writer'(ì‘ì„±ì)ë³„ë¡œ ëŒ€ëŒ“ê¸€ ìˆ˜, ì¢‹ì•„ìš” ìˆ˜ í•©ê³„ë¥¼ ë¶„ì„í•œ CSV.

        8. top_10_writers.csv / top_10_writers.png
           - ëŒ€ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ëª… ì‘ì„±ì ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³ , ë°” ê·¸ë˜í”„ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

        9. top_10_articles.csv / top_10_articles.png
           - ëŒ€ëŒ“ê¸€ ìˆ˜ ê¸°ì¤€ ìƒìœ„ 10ê°œ Article URLì„ ì •ë¦¬í•˜ê³ , ë°” ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

        10. top_10_liked_rereplies.csv
            - ì¢‹ì•„ìš”(Rereply Like)ê°€ ê°€ì¥ ë§ì€ ëŒ€ëŒ“ê¸€ 10ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        11. rereply_time_delta_distribution.png
            - (ëŒ€ëŒ“ê¸€ ì‘ì„± ë‚ ì§œ - ê²Œì‹œë¬¼ ì—…ë¡œë“œ ë‚ ì§œ)ë¥¼ ì¼(day) ë‹¨ìœ„ë¡œ ê³„ì‚°í•œ ì‹œì°¨ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.

        12. correlation_matrix.csv / correlation_matrix.png
            - 'Rereply Like', 'RereplyTimeDelta' ë“±ì˜ ìˆ˜ì¹˜ ì»¬ëŸ¼ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        """
        with open(safe_path(os.path.join(output_dir, "description.txt")), "w", encoding="utf-8", errors="ignore") as f:
            f.write(description_text)
        return True

    def wordcloud(self, parent, data, folder_path, date, max_words, split_option, exception_word_list, eng=False):
        parent = parent
        self.translate_history = {}
        self.translator = Translator()

        def divide_period(csv_data, period):
            # 'Unnamed' ì—´ ì œê±°
            csv_data = csv_data.loc[:, ~
                                    csv_data.columns.str.contains('^Unnamed')]

            # ë‚ ì§œ ì—´ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            csv_data[self.dateColumn_name] = pd.to_datetime(
                csv_data[self.dateColumn_name].str.split().str[0], format='%Y-%m-%d', errors='coerce')

            # 'YYYYMMDD' í˜•ì‹ì˜ ë¬¸ìì—´ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            start_date = pd.to_datetime(str(date[0]), format='%Y%m%d')
            end_date = pd.to_datetime(str(date[1]), format='%Y%m%d')

            # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
            csv_data = csv_data[csv_data[self.dateColumn_name].between(
                start_date, end_date)]

            if start_date < csv_data[self.dateColumn_name].min():
                self.startdate = int(
                    csv_data[self.dateColumn_name].min().strftime('%Y%m%d'))

            if end_date > csv_data[self.dateColumn_name].max():
                self.enddate = int(
                    csv_data[self.dateColumn_name].max().strftime('%Y%m%d'))

            if period == 'total':
                csv_data['period_group'] = 'total'
            else:
                # 'period_month' ì—´ ì¶”ê°€ (ì›” ë‹¨ìœ„ ê¸°ê°„ìœ¼ë¡œ ë³€í™˜)
                csv_data['period_month'] = csv_data[self.dateColumn_name].dt.to_period(
                    'M')

                # í•„ìš”í•œ ì „ì²´ ê¸°ê°„ ìƒì„±
                full_range = pd.period_range(start=csv_data['period_month'].min(), end=csv_data['period_month'].max(),
                                             freq='M')
                full_df = pd.DataFrame(full_range, columns=['period_month'])

                # ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•©í•˜ì—¬ ë¹ˆ ê¸°ê°„ë„ í¬í•¨í•˜ë„ë¡ í•¨
                csv_data = pd.merge(full_df, csv_data,
                                    on='period_month', how='left')

                # ìƒˆë¡œìš´ ì—´ì„ ì¶”ê°€í•˜ì—¬ ì£¼ê¸° ë‹¨ìœ„ë¡œ ê¸°ê°„ì„ ê·¸ë£¹í™”
                if period == '1m':  # ì›”
                    csv_data['period_group'] = csv_data['period_month'].astype(
                        str)
                elif period == '3m':  # ë¶„ê¸°
                    csv_data['period_group'] = (csv_data['period_month'].dt.year.astype(str) + 'Q' + (
                        (csv_data['period_month'].dt.month - 1) // 3 + 1).astype(str))
                elif period == '6m':  # ë°˜ê¸°
                    csv_data['period_group'] = (csv_data['period_month'].dt.year.astype(str) + 'H' + (
                        (csv_data['period_month'].dt.month - 1) // 6 + 1).astype(str))
                elif period == '1y':  # ì—°ë„
                    csv_data['period_group'] = csv_data['period_month'].dt.year.astype(
                        str)
                elif period == '1w':  # ì£¼
                    csv_data['period_group'] = csv_data[self.dateColumn_name].dt.to_period('W').apply(
                        lambda x: f"{x.start_time.strftime('%Y%m%d')}-{x.end_time.strftime('%Y%m%d')}"
                    )
                    first_date = csv_data['period_group'].iloc[0].split('-')[0]
                    end_date = csv_data['period_group'].iloc[-1].split('-')[1]
                    self.startdate = first_date
                    self.enddate = end_date
                elif period == '1d':  # ì¼
                    csv_data['period_group'] = csv_data[self.dateColumn_name].dt.to_period(
                        'D').astype(str)

            # ì£¼ê¸°ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê²°ê³¼ ë°˜í™˜
            period_divided_group = csv_data.groupby('period_group')

            return period_divided_group

        os.makedirs(os.path.join(folder_path, 'data'), exist_ok=True)

        for column in data.columns.tolist():
            if 'Text' in column:
                self.textColumn_name = column
            elif 'Date' in column:
                self.dateColumn_name = column

        print("\në°ì´í„° ë¶„í•  ì¤‘...\n")
        printStatus(parent, "ë°ì´í„° ë¶„í•  ì¤‘...")
        grouped = divide_period(data, split_option)
        period_list = list(grouped.groups.keys())

        i = 0

        if get_setting('ProcessConsole') == 'default':
            iterator = tqdm(grouped, desc="WordCloud ", file=sys.stdout,
                            bar_format="{l_bar}{bar}|", ascii=' =')
        else:
            iterator = grouped

        for period_start, group in iterator:
            printStatus(parent, f"wordcloud_{period_list[i]} ìƒì„± ì¤‘...")
            if group.empty:
                continue

            # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë³‘í•©
            all_words = []
            for tokens in group[self.textColumn_name]:
                if isinstance(tokens, str):  # í† í° ë¦¬ìŠ¤íŠ¸ê°€ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°
                    tokens = tokens.split(',')
                    all_words.extend(tokens)

            if exception_word_list != []:
                all_words = [
                    item.strip() for item in all_words if item.strip() not in exception_word_list]

            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            self.word_freq = dict(
                Counter(all_words).most_common(max_words))  # ë”•ì…”ë„ˆë¦¬ ë³€í™˜
            if eng == True:
                printStatus(parent, f"ë‹¨ì–´ ì˜ë¬¸ ë³€í™˜ ì¤‘...")
                asyncio.run(self.wordcloud_translator())

            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            wordcloud = WordCloud(font_path=os.path.join(os.path.dirname(
                __file__), '..', 'assets', 'malgun.ttf'), background_color='white', width=800, height=600, max_words=max_words)
            wc_generated = wordcloud.generate_from_frequencies(self.word_freq)

            # ì›Œë“œí´ë¼ìš°ë“œ ì €ì¥
            output_file = os.path.join(
                folder_path, f'wordcloud_{period_list[i]}.png')
            if split_option == 'total':
                output_file = os.path.join(
                    folder_path, f'wordcloud_{date[0]}~{date[1]}.png')

            wc_generated.to_file(output_file)

            # CSV íŒŒì¼ë¡œ ì €ì¥
            output_file = os.path.join(
                folder_path, 'data', f'wordcount_{period_list[i]}.csv')
            if split_option == 'total':
                output_file = os.path.join(
                    folder_path, 'data', f'wordcount_{date[0]}~{date[1]}.csv')

            with open(safe_path(output_file), mode="w", newline="", encoding="utf-8", errors="ignore") as file:
                writer = csv.writer(file)
                # í—¤ë” ì‘ì„±
                writer.writerow(["word", "count"])
                # ë°ì´í„° ì‘ì„±
                for word, count in self.word_freq.items():
                    writer.writerow([word, count])

            i += 1

    async def wordcloud_translator(self):
        translator = Translator()

        # ë²ˆì—­í•  í•œê¸€ ë‹¨ì–´ ëª©ë¡ (self.word_freqì˜ í‚¤ê°’ë“¤ ì¤‘ ë²ˆì—­ë˜ì§€ ì•Šì€ ë‹¨ì–´ë§Œ)
        word_dict = self.word_freq
        words_to_translate = [
            word for word in word_dict.keys() if word not in self.translate_history]

        # ë³‘ë ¬ ë²ˆì—­ ìˆ˜í–‰ (ì´ë¯¸ ë²ˆì—­ëœ ë‹¨ì–´ ì œì™¸)
        if words_to_translate:
            async def translate_word(word):
                """ ê°œë³„ ë‹¨ì–´ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë²ˆì—­í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ """
                result = await translator.translate(word, dest='en', src='auto')  # âœ… await ì¶”ê°€
                return word, result.text  # âœ… ë²ˆì—­ ê²°ê³¼ ë°˜í™˜

            # ë²ˆì—­ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
            translated_results = await asyncio.gather(*(translate_word(word) for word in words_to_translate))

            # ë²ˆì—­ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            for original, translated in translated_results:
                self.translate_history[original] = translated

        # ë³€í™˜ëœ word_freq ë”•ì…”ë„ˆë¦¬ ìƒì„± (ìºì‹œ í¬í•¨)
        self.word_freq = {k: v for k, v in sorted(
            {self.translate_history[word]: word_dict[word]
                for word in word_dict.keys()}.items(),
            key=lambda item: item[1],
            reverse=True
        )}

    def HateAnalysis(self, data: pd.DataFrame, file_path: str):
        """
        Hate / Clean / ë ˆì´ë¸” ì»¬ëŸ¼ì´ í¬í•¨ëœ CSVë¥¼ ë°›ì•„ ìë™ìœ¼ë¡œ
        option1 : Hate   ì—´ë§Œ ìˆìŒ
        option2 : 10ê°œ ë ˆì´ë¸”(ì—¬ì„±/ê°€ì¡±â€¥clean) ì—´ì´ ìˆìŒ
        option3 : Clean  ì—´ë§Œ ìˆìŒ
        ì„ íŒë³„í•˜ê³  â–¸ ê¸°ë³¸ í†µê³„ â–¸ ì›”Â·ì¼ë³„Â·7ì¼ Rolling í‰ê· 
        â–¸ ìƒìœ„ Top-N ê¸°ê°„ â–¸ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ â–¸ ë¶„í¬Â·ì¶”ì„¸ ê·¸ë˜í”„
        ë¥¼ `<ì›ë³¸>_hate_analysis/` í´ë”ì— ì €ì¥í•œë‹¤.
        """

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 0) ë‚ ì§œ ì—´ í™•ì¸
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        date_col = next((c for c in data.columns if "Date" in c), None)
        if date_col is None:
            QMessageBox.warning(self.main, "Warning", "'Date' ê°€ í¬í•¨ëœ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        data[date_col] = pd.to_datetime(data[date_col], errors="coerce")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1) ëª¨ë“œ íŒë³„ & ëŒ€ìƒ ì—´
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        LABEL_COLS = {
            "ì—¬ì„±/ê°€ì¡±", "ë‚¨ì„±", "ì„±ì†Œìˆ˜ì", "ì¸ì¢…/êµ­ì ",
            "ì—°ë ¹", "ì§€ì—­", "ì¢…êµ", "ê¸°íƒ€ í˜ì˜¤", "ì•…í”Œ/ìš•ì„¤", "clean",
        }
        mode, target_cols = None, []

        if "Hate" in data.columns:                 # option1
            mode, target_cols = 1, ["Hate"]

        else:
            present_lbl = [c for c in LABEL_COLS if c in data.columns]
            if len(present_lbl) >= 8:              # option2
                mode, target_cols = 2, present_lbl
            elif set(present_lbl) == {"clean"}:    # option3
                mode, target_cols = 3, ["clean"]

        if mode is None:
            QMessageBox.warning(self.main, "Warning", "Hate / Clean / ë ˆì´ë¸” ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2) ê²°ê³¼ í´ë”
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        out_dir   = os.path.join(
            os.path.dirname(file_path),
            os.path.splitext(os.path.basename(file_path))[0] + "_hate_analysis"
        )
        csv_dir   = os.path.join(out_dir, "csv_files")
        graph_dir = os.path.join(out_dir, "graphs")
        os.makedirs(csv_dir,   exist_ok=True)
        os.makedirs(graph_dir, exist_ok=True)

        def _safe_fname(label: str) -> str:
            return re.sub(r'[\\/*?:"<>|]', "_", label)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3) ê¸°ë³¸ í†µê³„
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        basic_stats = data[target_cols].describe()
        basic_stats.to_csv(os.path.join(csv_dir, "basic_stats.csv"), encoding="utf-8-sig")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4) ê¸°ê°„ë³„ í‰ê·  & 7-ì¼ Rolling
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        monthly = (
            data.groupby(data[date_col].dt.to_period("M"))[target_cols]
                .mean().reset_index()
        )
        monthly[date_col] = monthly[date_col].dt.to_timestamp()
        monthly.to_csv(os.path.join(csv_dir, "monthly_mean.csv"),
                    encoding="utf-8-sig", index=False)

        daily = (
            data.groupby(data[date_col].dt.to_period("D"))[target_cols]
                .mean().reset_index()
        )
        daily[date_col] = daily[date_col].dt.to_timestamp()
        daily.to_csv(os.path.join(csv_dir, "daily_mean.csv"),
                    encoding="utf-8-sig", index=False)

        # 7-ì¼ ì´ë™í‰ê· (íŠ¸ë Œë“œ ë¶€ë“œëŸ½ê²Œ ë³´ê¸°ìš©)
        rolling7 = (
            data.set_index(date_col)
                .sort_index()[target_cols]
                .rolling("7D").mean()
                .reset_index()
        )
        rolling7.to_csv(os.path.join(csv_dir, "rolling7_mean.csv"),
                        encoding="utf-8-sig", index=False)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 5) Top-N ê¸°ê°„ (ê°€ì¥ ë†’ì€ Hate/clean)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        topN = 10
        top_days = (
            daily.sort_values(target_cols[0], ascending=False)
                .head(topN)
        )
        top_days.to_csv(os.path.join(csv_dir, "top10_days.csv"),
                        encoding="utf-8-sig", index=False)

        top_months = (
            monthly.sort_values(target_cols[0], ascending=False)
                .head(topN)
        )
        top_months.to_csv(os.path.join(csv_dir, "top10_months.csv"),
                        encoding="utf-8-sig", index=False)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 6) ìƒê´€ê´€ê³„(ì˜µì…˜2 ì „ìš© ë˜ëŠ” Hate+Clean ë™ì‹œ ì¡´ì¬ ì‹œ)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        corr_cols = target_cols.copy()
        if "Hate" in data.columns and "clean" in data.columns:
            corr_cols = ["Hate", "clean"]
        if len(corr_cols) > 1:
            corr = data[corr_cols].corr()
            corr.to_csv(os.path.join(csv_dir, "correlation.csv"), encoding="utf-8-sig")

            plt.figure(figsize=self.calculate_figsize(len(corr), height=6))
            sns.heatmap(corr, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
            plt.title("Correlation Matrix")
            plt.tight_layout()
            plt.savefig(os.path.join(graph_dir, "correlation_heatmap.png"))
            plt.close()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 7) ê·¸ë˜í”„
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # (1) ì›”ë³„ ì¶”ì„¸
        plt.figure(figsize=self.calculate_figsize(len(monthly)))
        for col in target_cols[:6]:
            sns.lineplot(data=monthly, x=date_col, y=col, label=col)
        plt.title("ì›”ë³„ í‰ê·  ì ìˆ˜ ì¶”ì„¸")
        plt.xlabel("Month"); plt.ylabel("Mean Score")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_dir, "monthly_trend.png"))
        plt.close()

        # (2) 7-ì¼ ì´ë™ í‰ê·  ì¶”ì„¸(ë¶€ë“œëŸ¬ìš´ ë¼ì¸)
        plt.figure(figsize=self.calculate_figsize(len(rolling7)))
        for col in target_cols[:6]:
            sns.lineplot(data=rolling7, x=date_col, y=col, label=col)
        plt.title("7-Day Rolling Mean Trend")
        plt.xlabel("Date"); plt.ylabel("Rolling Mean")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(os.path.join(graph_dir, "rolling7_trend.png"))
        plt.close()

        # (3) ì ìˆ˜ ë¶„í¬
        for col in target_cols:
            plt.figure(figsize=self.calculate_figsize(10))
            sns.histplot(data[col], kde=True, bins=50)
            plt.title(f"{col} ì˜ì—­ í˜ì˜¤ë„ ë¶„í¬")
            plt.xlabel("Score"); plt.ylabel("Frequency")
            plt.tight_layout()
            plt.savefig(os.path.join(graph_dir, f"{_safe_fname(col)}_distribution.png"))
            plt.close()

        # (4) ë ˆì´ë¸” íˆíŠ¸ë§µ(option2 ì „ìš©)
        if mode == 2:
            heat_df = monthly.set_index(date_col)[target_cols]
            plt.figure(figsize=self.calculate_figsize(len(heat_df), height=8))
            sns.heatmap(
                heat_df.T,
                cmap="Reds", vmin=0, vmax=1,
                cbar_kws={"label": "ì›”ë³„ í‰ê·  í™•ë¥ "}
            )
            plt.title("ì›”ë³„ ë ˆì´ë¸” í‰ê·  íˆíŠ¸ë§µ")
            plt.tight_layout()
            plt.savefig(os.path.join(graph_dir, "label_heatmap.png"))
            plt.close()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 8) ì„¤ëª… í…ìŠ¤íŠ¸
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        desc = [
            "â˜… í˜ì˜¤ í†µê³„ ë¶„ì„ ê²°ê³¼ ì•ˆë‚´",
            "",
            f"ìë™ íŒë³„ëœ ëª¨ë“œ  : option {mode}",
            "  option1 : Hate ì—´",
            "  option2 : 10ê°œ ë ˆì´ë¸” ì—´",
            "  option3 : Clean ì—´",
            "",
            "â–  CSV",
            "  Â· basic_stats.csv      : ê¸°ì´ˆ í†µê³„",
            "  Â· monthly_mean.csv     : ì›”ë³„ í‰ê· ",
            "  Â· daily_mean.csv       : ì¼ë³„ í‰ê· ",
            "  Â· rolling7_mean.csv    : 7-ì¼ ì´ë™ í‰ê· ",
            "  Â· top10_days / months  : ê°€ì¥ ë†’ì€ ì ìˆ˜ TOP 10",
            "  Â· correlation.csv      : ìƒê´€ê´€ê³„(í•´ë‹¹ ì‹œ)",
            "",
            "â–  Graphs",
            "  Â· monthly_trend.png    : ì›”ë³„ ì¶”ì„¸",
            "  Â· rolling7_trend.png   : 7-ì¼ ì´ë™ í‰ê·  ì¶”ì„¸",
            "  Â· *_distribution.png   : ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨",
            "  Â· correlation_heatmap.png : ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ(í•´ë‹¹ ì‹œ)",
            "  Â· label_heatmap.png    : ë ˆì´ë¸” íˆíŠ¸ë§µ(option2)",
        ]
        with open(safe_path(os.path.join(out_dir, "description.txt")), "w",
                encoding="utf-8", errors="ignore") as f:
            f.write("\n".join(desc))
