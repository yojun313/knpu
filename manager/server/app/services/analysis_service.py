from app.models.analysis_model import *
from app.libs.kemkim import KimKem
from app.libs.progress import *
import os
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import shutil
from fastapi.responses import FileResponse, JSONResponse
from starlette.background import BackgroundTask
from kiwipiepy import Kiwi
import time
import pandas as pd
import re
from kiwipiepy import Kiwi
import torch, numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TextClassificationPipeline,
    logging
)
logging.set_verbosity_error()
import os
from dotenv import load_dotenv
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from app.utils.zip import fast_zip
import gc

load_dotenv() 

kor_unsmile_pipe = None
topic_model = None
kiwi_instance = None

def get_kiwi():
    global kiwi_instance
    if kiwi_instance is None:
        kiwi_instance = Kiwi(num_workers=-1)
    return kiwi_instance

def get_hate_model():
    global kor_unsmile_pipe, topic_model
    MODEL_DIR = os.getenv("MODEL_PATH")
    
    if kor_unsmile_pipe is None:
        tokenizer = AutoTokenizer.from_pretrained(os.path.join(MODEL_DIR, "kor_unsmile"), local_files_only=True)
        kor_unsmile_model = AutoModelForSequenceClassification.from_pretrained(os.path.join(MODEL_DIR, "kor_unsmile"), local_files_only=True)
        kor_unsmile_pipe = TextClassificationPipeline(
            model=kor_unsmile_model,
            tokenizer=tokenizer,
            function_to_apply="sigmoid",
            top_k=None,
            device=0 if torch.cuda.is_available() else -1,
        )

    if topic_model is None:
        embed_model = SentenceTransformer(os.path.join(MODEL_DIR, "topic"),
                                          device="cuda" if torch.cuda.is_available() else "cpu")
        topic_model = KeyBERT(embed_model)

    return kor_unsmile_pipe, topic_model

def unload_hate_model():
    global kor_unsmile_pipe, topic_model
    kor_unsmile_pipe = None
    topic_model = None
    torch.cuda.empty_cache()
    gc.collect()

def start_kemkim(option: KemKimOption, token_data):

    def cleanup_folder_and_zip(folder_path: str, zip_path: str):
        shutil.rmtree(folder_path, ignore_errors=True)
        try:
            os.remove(zip_path)
        except OSError:
            pass

    option = option.model_dump()
    save_path = os.path.join(os.path.dirname(__file__), '..', 'temp')

    kemkim_obj = KimKem(
        pid=option["pid"],
        token_data=token_data,
        csv_name=option["tokenfile_name"],
        save_path=save_path,
        startdate=option["startdate"],
        enddate=option["enddate"],
        period=option["period"],
        topword=option["topword"],
        weight=option["weight"],
        graph_wordcnt=option["graph_wordcnt"],
        split_option=option["split_option"],
        split_custom=option["split_custom"],
        filter_option=option["filter_option"],
        trace_standard=option["trace_standard"],
        ani_option=option["ani_option"],
        exception_word_list=option["exception_word_list"],
        exception_filename=option["exception_filename"],
        modify_kemkim=False
    )
    try:
        result_path = kemkim_obj.make_kimkem()
        
        if type(result_path) == str:
            zip_path = f"{result_path}.zip"
            fast_zip(result_path, zip_path)   
            filename = os.path.basename(zip_path)

            background_task = BackgroundTask(
                cleanup_folder_and_zip, result_path, zip_path)

            # 4) FileResponseì— filename= ìœ¼ë¡œ ë„˜ê¸°ê¸°
            return FileResponse(
                path=zip_path,
                media_type="application/zip",
                filename=filename,
                background=background_task,
            )
        elif result_path == 2:
            # ì˜ˆì™¸ ìƒí™© ë©”ì‹œì§€ ì‘ë‹µ
            return JSONResponse(
                status_code=400,
                content={"error": "KEMKIM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                         "message": "ì‹œê°„ ê°€ì¤‘ì¹˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"}
            )
        elif result_path == 3:
            # ì˜ˆì™¸ ìƒí™© ë©”ì‹œì§€ ì‘ë‹µ
            return JSONResponse(
                status_code=400,
                content={"error": "KEMKIM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                         "message": "í‚¤ì›Œë“œê°€ ì—†ì–´ ë¶„ì„ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"}
            )

    except Exception as e:
        # ì˜ˆì™¸ ìƒí™© ë©”ì‹œì§€ ì‘ë‹µ
        return JSONResponse(
            status_code=500,
            content={"error": "KEMKIM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ", "message": str(e)}
        )

def tokenization(
    pid: str,
    data: pd.DataFrame,
    columns,
    include_words: list = None,
    update_interval: int = 3000,
) -> pd.DataFrame:
    """
    â–¸ pid            : ì§„í–‰ ìƒí™©ì„ send_message(pid, â€¦)ë¡œ ì „ë‹¬í•  ë•Œ ì‚¬ìš©
    â–¸ data           : ì›ë³¸ DataFrame (in-place ìˆ˜ì •)
    â–¸ columns        : í† í°í™”í•  ì—´ ì´ë¦„ ë˜ëŠ” ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    â–¸ update_interval: ì´ ê°œìˆ˜ë§ˆë‹¤ ì§„í–‰ë¥  ë©”ì‹œì§€ ì „ì†¡
    """
    # Kiwi í•œ ë²ˆë§Œ ì´ˆê¸°í™”
    kiwi = get_kiwi()
    for word in include_words:
        kiwi.add_user_word(word, 'NNP', score=10)

    # ë‹¨ì¼ str â†’ list
    if isinstance(columns, str):
        columns = [columns]

    # ê° ì—´ì„ ìˆœíšŒ
    for col in columns:
        if col not in data.columns:
            send_message(pid, f"âš ï¸  ì—´ '{col}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ â†’ ê±´ë„ˆëœ€")
            continue

        texts        = data[col].tolist()
        total        = len(texts)
        tokenized_col = []

        send_message(pid, f"[{col}] í† í°í™” ì‹œì‘ (ì´ {total:,} rows)")

        total_time = 0.0
        for idx, text in enumerate(texts, 1):
            start = time.time()

            if isinstance(text, str):
                # ì „ì²˜ë¦¬
                cleaned = re.sub(r"[^ê°€-í£a-zA-Z\s]", "", text)
                # splitComplex=False â†’ ë³µí•©ì–´ë¥¼ ë¶„í•´í•˜ì§€ ì•Šê³  ì²˜ë¦¬
                tokens   = kiwi.tokenize(cleaned, split_complex=False)
                nouns    = [t.form for t in tokens if t.tag in ("NNG", "NNP")]
                tokenized_col.append(", ".join(nouns))
            else:
                tokenized_col.append("")

            # ì§„í–‰ë¥  ê³„ì‚°
            total_time += time.time() - start
            if idx % update_interval == 0 or idx == total:
                pct   = round(idx / total * 100, 2)
                avg   = total_time / idx
                remain_sec = avg * (total - idx)
                m, s  = divmod(int(remain_sec), 60)
                send_message(
                    pid,
                    f"[{col}] ì§„í–‰ë¥  {pct}% ({idx:,}/{total:,}) â€¢ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ {m}ë¶„ {s}ì´ˆ"
                )

        # ì—´ ë®ì–´ì“°ê¸°
        data[col] = tokenized_col
        send_message(pid, f"[{col}] í† í°í™” ì™„ë£Œ")

    return data

def measure_hate(
    option: HateOption,
    data: pd.DataFrame,
    text_col: str | None = "Text",
    update_interval: int = 1_000,
    batch_size: int = 32,
) -> pd.DataFrame:
    """
    option.option_num
      1 â†’ clean ì œì™¸ ë ˆì´ë¸” ì¤‘ ìµœëŒ€ê°’ â†’ Hate ì—´
      2 â†’ 10ê°œ ë ˆì´ë¸” ëª¨ë‘         â†’ ë ˆì´ë¸”ëª…ë³„ ì—´
      3 â†’ clean í™•ë¥                â†’ Clean ì—´
    """

    def batch_scores(texts: list[str]) -> list[dict[str, float]]:
        """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ â†’ [{label: prob}, ...] (ë‘˜ì§¸ ìë¦¬ ë°˜ì˜¬ë¦¼)"""
        pipe, _ = get_hate_model()
        outs = pipe(
            texts,
            truncation=True,
            batch_size=batch_size,
        )
        return [
            {o["label"]: round(o["score"], 2) for o in each}
            for each in outs
        ]


    pid, mode = option.pid, option.option_num

    # ëŒ€ìƒ ì—´ íƒìƒ‰ 
    if text_col not in data.columns:
        for c in data.columns:
            if "text" in c.lower():
                text_col = c
                send_message(pid, f"ğŸ” '{text_col}' ì—´ ìë™ ì„ íƒ")
                break
        else:
            raise ValueError("'Text'ë¼ëŠ” ê¸€ìë¥¼ í¬í•¨í•œ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    texts = data[text_col].fillna("").astype(str).tolist()
    total = len(texts)
    pipe, _ = get_hate_model()
    labels = list(pipe.model.config.id2label.values())
    
    send_message(pid, f"[í˜ì˜¤ë„ ë¶„ì„] '{text_col}' ì²˜ë¦¬ ì‹œì‘ (ì´ {total:,} rows)")

    # ê²°ê³¼ ë²„í¼ 
    if mode == 1:
        results = [0.0] * total
    elif mode == 2:
        # dict ëŒ€ì‹  numpy arrayë¥¼ ì“°ëŠ” ê²ƒë„ ì„±ëŠ¥ í–¥ìƒì— ì¢‹ìŒ
        results = {lbl: [0.0] * total for lbl in labels}
    elif mode == 3:
        results = [0.0] * total
    else:
        raise ValueError("option_num must be 1, 2, ë˜ëŠ” 3 ì´ì–´ì•¼ í•©ë‹ˆë‹¤")

    # ë¯¸ë¦¬ ë¹„ì–´ìˆì§€ ì•Šì€ ì¸ë±ìŠ¤ í•„í„°ë§ 
    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]
    non_empty_texts = [texts[i].strip() for i in non_empty_indices]
    total_non_empty = len(non_empty_indices)

    # ë°°ì¹˜ ì¶”ë¡  
    for batch_start in range(0, total_non_empty, batch_size):
        batch_end = min(batch_start + batch_size, total_non_empty)
        batch_idx = non_empty_indices[batch_start:batch_end]
        batch_txt = non_empty_texts[batch_start:batch_end]

        # ëª¨ë¸ í•œ ë²ˆ í˜¸ì¶œ
        scored = batch_scores(batch_txt)

        # ê²°ê³¼ ì±„ìš°ê¸°
        if mode == 1:
            for idx, sc in zip(batch_idx, scored):
                results[idx] = max(v for k, v in sc.items() if k != "clean")
        elif mode == 2:
            for idx, sc in zip(batch_idx, scored):
                for lbl in labels:
                    results[lbl][idx] = sc.get(lbl, 0.0)
        else:  # mode == 3
            for idx, sc in zip(batch_idx, scored):
                results[idx] = sc.get("clean", 0.0)

        if (batch_end % update_interval == 0) or (batch_end == total_non_empty):
            pct = round(batch_end / total_non_empty * 100, 2)
            send_message(pid, f"[í˜ì˜¤ë„ ë¶„ì„] {pct}% ì™„ë£Œ ({batch_end:,}/{total_non_empty:,})")

    # ê²°ê³¼ ì—´ ë¶™ì´ê¸°
    if mode == 1:
        data["Hate"] = results
    elif mode == 2:
        for lbl, vals in results.items():
            data[lbl] = vals
    else:
        data["Clean"] = results

    send_message(pid, "[í˜ì˜¤ë„ ë¶„ì„] ì™„ë£Œ")
    unload_hate_model()
    return data

def extract_keywords(
    pid: str,
    data: pd.DataFrame,
    text_col: str = "Text",
    top_n: int = 5,
    update_interval: int = 1_000,
) -> pd.DataFrame:
    """
    â–¸ pid         : ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ì „ì†¡ìš© ID
    â–¸ data        : ì›ë³¸ DataFrame
    â–¸ text_col    : í‚¤ì›Œë“œ ì¶”ì¶œ ëŒ€ìƒ ì»¬ëŸ¼
    â–¸ top_n       : ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
    â–¸ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì—¬ í›„ë³´ì–´ë¡œ ì‚¬ìš©
    """

    import re

    def split_sentences(text, max_len=512):
        # ê°„ë‹¨í•œ ë¬¸ì¥ ë‹¨ìœ„ split (ì •ì œ í•„ìš”ì‹œ ë” ì •êµí•˜ê²Œ ê°€ëŠ¥)
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current = ""
        for sent in sentences:
            if len(current) + len(sent) < max_len:
                current += " " + sent
            else:
                chunks.append(current.strip())
                current = sent
        if current:
            chunks.append(current.strip())
        return chunks

    # ëŒ€ìƒ ì—´ íƒìƒ‰ 
    if text_col not in data.columns:
        matches = [c for c in data.columns if "text" in c.lower()]
        if not matches:
            raise ValueError("'Text'ë¼ëŠ” ê¸€ìë¥¼ í¬í•¨í•œ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        text_col = matches[0]
        send_message(pid, f"'{text_col}' ì—´ ìë™ ì„ íƒ")

    texts = data[text_col].fillna("").astype(str).tolist()
    total = len(texts)

    send_message(pid, f"[í† í”½ ë¶„ì„] '{text_col}' ì²˜ë¦¬ ì‹œì‘ (ì´ {total:,} rows)")

    # Kiwi ì´ˆê¸°í™”
    kiwi = get_kiwi()

    keywords_col = [""] * total

    # ë£¨í”„ ì‹œì‘ 
    for idx, text in enumerate(texts, 1):
        cleaned = text.strip()
        if cleaned:
            try:
                # ê¸´ í…ìŠ¤íŠ¸ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                chunks = split_sentences(cleaned, max_len=500)

                all_keywords = []
                for chunk in chunks:
                    tokens = kiwi.tokenize(chunk, split_complex=False)
                    noun_candidates = [t.form for t in tokens if t.tag in ("NNG", "NNP")]

                    # í›„ë³´ì–´ ë„ˆë¬´ ë§ìœ¼ë©´ ìë¥´ê¸°
                    noun_candidates = noun_candidates[:500]

                    if noun_candidates:
                        _, topic_model = get_hate_model()
                        kw = topic_model.extract_keywords(
                            chunk,
                            candidates=noun_candidates,
                            keyphrase_ngram_range=(1, 2),
                            stop_words=None,
                            top_n=top_n,
                            use_mmr=True,
                            diversity=0.7
                        )
                        all_keywords.extend([k[0] for k in kw])

                keywords_col[idx - 1] = ", ".join(list(dict.fromkeys(all_keywords)))  # ì¤‘ë³µ ì œê±°
            except Exception as e:
                print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜ ({idx}):", e)
                keywords_col[idx - 1] = ""

        # ì§„í–‰ë¥  ì¶œë ¥
        if idx % update_interval == 0 or idx == total:
            pct = round(idx / total * 100, 2)
            send_message(pid, f"[í† í”½ ë¶„ì„] {pct}% ì™„ë£Œ ({idx:,}/{total:,})")

    # ê²°ê³¼ ì—´ ì¶”ê°€
    data["Keywords"] = keywords_col
    send_message(pid, "[í† í”½ ë¶„ì„] ì™„ë£Œ")
    unload_hate_model()
    return data

