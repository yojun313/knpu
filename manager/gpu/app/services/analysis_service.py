import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
from app.models.analysis_model import *
from app.libs.progress import *
import os
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import pandas as pd
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TextClassificationPipeline,
    logging
)
logging.set_verbosity_error()
import os
from faster_whisper import WhisperModel
from dotenv import load_dotenv
import gc

load_dotenv() 
MODEL_DIR = os.getenv("MODEL_PATH")

kor_unsmile_pipe = None
_whisper_models = {}

WHISPER_MODEL_MAP = {
    1: {
        "name": "faster-whisper-small",
        "compute": "int8_float16",
    },
    2: {
        "name": "faster-whisper-medium",
        "compute": "int8_float16",
    },
    3: {
        "name": "faster-whisper-large-v3",
        "compute": "float16",
    },
}



whisper_model = WhisperModel(
    os.path.join(MODEL_DIR, "faster-whisper-large-v3"),
    device="cuda",
    compute_type="float16",
    local_files_only=True,
)

def get_hate_model():
    global kor_unsmile_pipe
    
    if kor_unsmile_pipe is None:
        tokenizer = AutoTokenizer.from_pretrained(os.path.join(MODEL_DIR, "kor_unsmile"), local_files_only=True)
        kor_unsmile_model = AutoModelForSequenceClassification.from_pretrained(os.path.join(MODEL_DIR, "kor_unsmile"), local_files_only=True)
        kor_unsmile_pipe = TextClassificationPipeline(
            model=kor_unsmile_model,
            tokenizer=tokenizer,
            function_to_apply="sigmoid",
            top_k=None,
            device=1 if torch.cuda.is_available() else -1,
        )

    return kor_unsmile_pipe

def get_whisper_model(level: int):
    if level not in WHISPER_MODEL_MAP:
        level = 2  # ê¸°ë³¸ê°’ medium

    cfg = WHISPER_MODEL_MAP[level]

    key = f"{cfg['name']}::{cfg['compute']}"

    if key not in _whisper_models:
        _whisper_models[key] = WhisperModel(
            os.path.join(MODEL_DIR, cfg["name"]),
            device="cuda",
            compute_type=cfg["compute"],
            local_files_only=True,
        )

    return _whisper_models[key]


def unload_hate_model():
    global kor_unsmile_pipe
    kor_unsmile_pipe = None
    torch.cuda.empty_cache()
    gc.collect()

def measure_hate(
    option: HateOption,
    data: pd.DataFrame,
    text_col: str | None = "Text",
    update_interval: int = 1_000,
    batch_size: int = 64,
) -> pd.DataFrame:
    """
    option.option_num
      1 â†’ clean ì œì™¸ ë ˆì´ë¸” ì¤‘ ìµœëŒ€ê°’ â†’ Hate ì—´
      2 â†’ 10ê°œ ë ˆì´ë¸” ëª¨ë‘         â†’ ë ˆì´ë¸”ëª…ë³„ ì—´
      3 â†’ clean í™•ë¥                â†’ Clean ì—´
    """

    def batch_scores(texts: list[str]) -> list[dict[str, float]]:
        """ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ â†’ [{label: prob}, ...] (ë‘˜ì§¸ ìë¦¬ ë°˜ì˜¬ë¦¼)"""
        pipe = get_hate_model()
        outs = pipe(
            texts,
            truncation=True,
            batch_size=batch_size,
        )
        return [
            {o["label"]: round(o["score"], 2) for o in each}
            for each in outs
        ]


    pid, mode = option.pid, option.option_num

    # ëŒ€ìƒ ì—´ íƒìƒ‰ 
    if text_col not in data.columns:
        for c in data.columns:
            if "text" in c.lower():
                text_col = c
                send_message(pid, f"ğŸ” '{text_col}' ì—´ ìë™ ì„ íƒ")
                break
        else:
            raise ValueError("'Text'ë¼ëŠ” ê¸€ìë¥¼ í¬í•¨í•œ ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    texts = data[text_col].fillna("").astype(str).tolist()
    total = len(texts)
    pipe = get_hate_model()
    labels = list(pipe.model.config.id2label.values())
    
    send_message(pid, f"[í˜ì˜¤ë„ ë¶„ì„] '{text_col}' ì²˜ë¦¬ ì‹œì‘ (ì´ {total:,} rows)")

    # ê²°ê³¼ ë²„í¼ 
    if mode == 1:
        results = [0.0] * total
    elif mode == 2:
        # dict ëŒ€ì‹  numpy arrayë¥¼ ì“°ëŠ” ê²ƒë„ ì„±ëŠ¥ í–¥ìƒì— ì¢‹ìŒ
        results = {lbl: [0.0] * total for lbl in labels}
    elif mode == 3:
        results = [0.0] * total
    else:
        raise ValueError("option_num must be 1, 2, ë˜ëŠ” 3 ì´ì–´ì•¼ í•©ë‹ˆë‹¤")

    # ë¯¸ë¦¬ ë¹„ì–´ìˆì§€ ì•Šì€ ì¸ë±ìŠ¤ í•„í„°ë§ 
    non_empty_indices = [i for i, t in enumerate(texts) if t.strip()]
    non_empty_texts = [texts[i].strip() for i in non_empty_indices]
    total_non_empty = len(non_empty_indices)

    # ë°°ì¹˜ ì¶”ë¡  
    for batch_start in range(0, total_non_empty, batch_size):
        batch_end = min(batch_start + batch_size, total_non_empty)
        batch_idx = non_empty_indices[batch_start:batch_end]
        batch_txt = non_empty_texts[batch_start:batch_end]

        # ëª¨ë¸ í•œ ë²ˆ í˜¸ì¶œ
        scored = batch_scores(batch_txt)

        # ê²°ê³¼ ì±„ìš°ê¸°
        if mode == 1:
            for idx, sc in zip(batch_idx, scored):
                results[idx] = max(v for k, v in sc.items() if k != "clean")
        elif mode == 2:
            for idx, sc in zip(batch_idx, scored):
                for lbl in labels:
                    results[lbl][idx] = sc.get(lbl, 0.0)
        else:  # mode == 3
            for idx, sc in zip(batch_idx, scored):
                results[idx] = sc.get("clean", 0.0)

        if (batch_end % update_interval == 0) or (batch_end == total_non_empty):
            pct = round(batch_end / total_non_empty * 100, 2)
            send_message(pid, f"[í˜ì˜¤ë„ ë¶„ì„] {pct}% ì™„ë£Œ ({batch_end:,}/{total_non_empty:,})")

    # ê²°ê³¼ ì—´ ë¶™ì´ê¸°
    if mode == 1:
        data["Hate"] = results
    elif mode == 2:
        for lbl, vals in results.items():
            data[lbl] = vals
    else:
        data["Clean"] = results

    send_message(pid, "[í˜ì˜¤ë„ ë¶„ì„] ì™„ë£Œ")
    unload_hate_model()
    return data

def format_paragraphs(segments, max_len=120):
    paragraphs = []
    buf = ""

    for seg in segments:
        text = seg.text.strip()
        if not text:
            continue

        if len(buf) + len(text) <= max_len:
            buf += " " + text
        else:
            paragraphs.append(buf.strip())
            buf = text

    if buf:
        paragraphs.append(buf.strip())

    return "\n\n".join(paragraphs)


def format_with_timestamps(segments):
    def ts(t):
        h = int(t // 3600)
        m = int((t % 3600) // 60)
        s = int(t % 60)
        ms = int((t - int(t)) * 1000)
        return f"{h:02}:{m:02}:{s:02},{ms:03}"

    lines = []
    for seg in segments:
        line = f"[{ts(seg.start)} - {ts(seg.end)}] {seg.text.strip()}"
        lines.append(line)

    return "\n".join(lines)


def transcribe_audio(
    audio_path: str,
    language: str = "ko",
    model_level: int = 2,
    pid = None,
):
    send_message(pid, f"[ìŒì„± ì¸ì‹] {WHISPER_MODEL_MAP[model_level]['name']} ëª¨ë¸ ë¡œë“œ ì¤‘")
    model = get_whisper_model(model_level)

    send_message(pid, "[ìŒì„± ì¸ì‹] Audio -> Text ë³€í™˜ ì¤‘")
    segments, info = model.transcribe(
        audio_path,
        language=language,
        beam_size=1 if model_level < 3 else 5,
        vad_filter=True,
    )

    segments = list(segments)

    text_paragraph = format_paragraphs(segments)
    text_with_time = format_with_timestamps(segments)
    
    send_message(pid, "[ìŒì„± ì¸ì‹] ì™„ë£Œ")
    return {
        "language": info.language,
        "duration": info.duration,
        "model_level": model_level,
        "text": text_paragraph,
        "text_with_time": text_with_time,
        "segments": [
            {
                "start": seg.start,
                "end": seg.end,
                "text": seg.text.strip(),
            }
            for seg in segments
        ],
    }
